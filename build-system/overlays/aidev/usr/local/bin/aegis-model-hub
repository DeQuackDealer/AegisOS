#!/usr/bin/env python3
"""
Aegis Model Hub - Hugging Face cache and model download management
Features: HuggingFace cache config, Ollama models, model download tool

Provides GUI (GTK) and CLI modes with tier-based feature gating.
"""

import os
import sys
import json
import subprocess
import logging
import argparse
import shutil
import hashlib
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, asdict

TIER_LIMIT = "aidev"
VERSION = "1.5.0"
APP_NAME = "Aegis Model Hub"

CONFIG_FILE = "/etc/aegis/aidev-config.json"
LOG_FILE = "/var/log/aegis/model-hub.log"

try:
    import gi
    gi.require_version('Gtk', '3.0')
    from gi.repository import Gtk, GLib
    GTK_AVAILABLE = True
except ImportError:
    GTK_AVAILABLE = False
    print("Error: GTK3 is required. Install with: sudo pacman -S gtk3 python-gobject", file=sys.stderr)


class LicenseTier:
    FREEMIUM = 1
    BASIC = 2
    AIDEV = 4
    SERVER = 5


@dataclass
class ModelInfo:
    name: str
    source: str
    size_gb: float
    path: str
    downloaded: bool
    last_used: Optional[str]


POPULAR_MODELS = {
    "llama-3.2-3b": {"source": "ollama", "size_gb": 2.0, "description": "Meta's Llama 3.2 3B"},
    "llama-3.2-1b": {"source": "ollama", "size_gb": 1.3, "description": "Meta's Llama 3.2 1B"},
    "mistral-7b": {"source": "ollama", "size_gb": 4.1, "description": "Mistral 7B Instruct"},
    "codellama-7b": {"source": "ollama", "size_gb": 3.8, "description": "Code Llama 7B"},
    "phi-3-mini": {"source": "ollama", "size_gb": 2.3, "description": "Microsoft Phi-3 Mini"},
    "gemma-2b": {"source": "ollama", "size_gb": 1.4, "description": "Google Gemma 2B"},
    "qwen2-7b": {"source": "ollama", "size_gb": 4.4, "description": "Alibaba Qwen2 7B"},
    "deepseek-coder-6.7b": {"source": "ollama", "size_gb": 3.8, "description": "DeepSeek Coder 6.7B"}
}

HF_MODELS = {
    "bert-base": {"repo": "bert-base-uncased", "size_gb": 0.4},
    "gpt2": {"repo": "gpt2", "size_gb": 0.5},
    "t5-base": {"repo": "t5-base", "size_gb": 0.9},
    "sentence-transformers": {"repo": "sentence-transformers/all-MiniLM-L6-v2", "size_gb": 0.1},
    "whisper-base": {"repo": "openai/whisper-base", "size_gb": 0.3}
}


class AegisModelHub:
    def __init__(self, headless: bool = False):
        self.headless = headless
        self.version = VERSION
        self.config = {}
        self.license_tier = LicenseTier.FREEMIUM
        
        self.setup_logging()
        self.load_license_tier()
        self.load_config()
        
    def setup_logging(self):
        log_dir = Path(LOG_FILE).parent
        try:
            log_dir.mkdir(parents=True, exist_ok=True)
        except PermissionError:
            pass
        
        try:
            logging.basicConfig(
                level=logging.INFO,
                format='%(asctime)s - %(levelname)s - [%(name)s] %(message)s',
                handlers=[
                    logging.FileHandler(LOG_FILE) if os.access(str(log_dir), os.W_OK) else logging.NullHandler(),
                    logging.StreamHandler() if not self.headless else logging.NullHandler()
                ]
            )
        except Exception:
            logging.basicConfig(level=logging.INFO, handlers=[logging.StreamHandler()])
        
        self.logger = logging.getLogger("AegisModelHub")
    
    def load_license_tier(self):
        license_file = Path("/etc/aegis/license.json")
        try:
            if license_file.exists():
                with open(license_file, 'r') as f:
                    license_data = json.load(f)
                edition = license_data.get('edition', 'freemium').lower()
                tier_map = {'freemium': LicenseTier.FREEMIUM, 'aidev': LicenseTier.AIDEV}
                self.license_tier = tier_map.get(edition, LicenseTier.FREEMIUM)
            elif Path("/etc/aegis-aidev-marker").exists():
                self.license_tier = LicenseTier.AIDEV
        except Exception as e:
            self.logger.warning(f"Failed to load license tier: {e}")
    
    def is_feature_available(self, feature: str) -> bool:
        return self.license_tier >= LicenseTier.AIDEV
    
    def load_config(self):
        default_config = {
            "huggingface_cache": str(Path.home() / ".cache" / "huggingface"),
            "ollama_models_dir": str(Path.home() / ".ollama" / "models"),
            "max_cache_size_gb": 100
        }
        
        try:
            if Path(CONFIG_FILE).exists():
                with open(CONFIG_FILE, 'r') as f:
                    file_config = json.load(f)
                    if "features" in file_config and "model_hub" in file_config["features"]:
                        self.config = {**default_config, **file_config["features"]["model_hub"]}
                    else:
                        self.config = default_config
            else:
                self.config = default_config
        except Exception as e:
            self.logger.error(f"Error loading config: {e}")
            self.config = default_config
    
    def check_ollama_installed(self) -> bool:
        return shutil.which("ollama") is not None
    
    def list_ollama_models(self) -> List[Dict[str, Any]]:
        if not self.check_ollama_installed():
            return []
        
        try:
            result = subprocess.run(["ollama", "list"], capture_output=True, text=True, timeout=30)
            if result.returncode == 0:
                models = []
                lines = result.stdout.strip().split('\n')[1:]
                for line in lines:
                    parts = line.split()
                    if parts:
                        models.append({"name": parts[0], "size": parts[1] if len(parts) > 1 else "unknown"})
                return models
        except Exception as e:
            self.logger.error(f"Failed to list Ollama models: {e}")
        return []
    
    def download_ollama_model(self, model_name: str) -> Dict[str, Any]:
        if not self.is_feature_available("model_download"):
            return {"success": False, "error": "Model download requires AI Developer edition"}
        
        if not self.check_ollama_installed():
            return {"success": False, "error": "Ollama not installed"}
        
        try:
            self.logger.info(f"Downloading model: {model_name}")
            result = subprocess.run(
                ["ollama", "pull", model_name],
                capture_output=True, text=True, timeout=3600
            )
            
            if result.returncode == 0:
                return {"success": True, "model": model_name}
            else:
                return {"success": False, "error": result.stderr}
        except subprocess.TimeoutExpired:
            return {"success": False, "error": "Download timed out"}
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def delete_ollama_model(self, model_name: str) -> bool:
        if not self.check_ollama_installed():
            return False
        
        try:
            result = subprocess.run(["ollama", "rm", model_name], capture_output=True, timeout=60)
            return result.returncode == 0
        except Exception:
            return False
    
    def get_cache_size(self) -> Dict[str, float]:
        sizes = {"huggingface": 0.0, "ollama": 0.0, "total": 0.0}
        
        hf_cache = Path(self.config.get("huggingface_cache", "")).expanduser()
        if hf_cache.exists():
            try:
                size = sum(f.stat().st_size for f in hf_cache.rglob('*') if f.is_file())
                sizes["huggingface"] = round(size / (1024**3), 2)
            except Exception:
                pass
        
        ollama_dir = Path(self.config.get("ollama_models_dir", "")).expanduser()
        if ollama_dir.exists():
            try:
                size = sum(f.stat().st_size for f in ollama_dir.rglob('*') if f.is_file())
                sizes["ollama"] = round(size / (1024**3), 2)
            except Exception:
                pass
        
        sizes["total"] = sizes["huggingface"] + sizes["ollama"]
        return sizes
    
    def get_status(self) -> Dict[str, Any]:
        return {
            "ollama_installed": self.check_ollama_installed(),
            "ollama_models": self.list_ollama_models(),
            "cache_size": self.get_cache_size(),
            "tier": self.license_tier
        }
    
    def run_gui(self):
        if not GTK_AVAILABLE:
            return self.run_cli()
        
        win = ModelHubWindow(self)
        win.connect("destroy", Gtk.main_quit)
        win.show_all()
        Gtk.main()
    
    def run_cli(self):
        print(f"\n{'='*60}")
        print(f"  {APP_NAME} v{VERSION}")
        print(f"{'='*60}\n")
        
        status = self.get_status()
        cache = status["cache_size"]
        
        print(f"Ollama: {'✓ Installed' if status['ollama_installed'] else '✗ Not installed'}")
        print(f"\nCache Usage:")
        print(f"  HuggingFace: {cache['huggingface']} GB")
        print(f"  Ollama: {cache['ollama']} GB")
        print(f"  Total: {cache['total']} GB")
        
        if status["ollama_models"]:
            print(f"\nInstalled Ollama Models:")
            for model in status["ollama_models"]:
                print(f"  - {model['name']} ({model['size']})")
        
        print("\nAvailable Models:")
        for name, info in list(POPULAR_MODELS.items())[:5]:
            print(f"  - {name}: {info['description']} ({info['size_gb']} GB)")


if GTK_AVAILABLE:
    class ModelHubWindow(Gtk.Window):
        def __init__(self, app: AegisModelHub):
            super().__init__(title=f"{APP_NAME} v{VERSION}")
            self.app = app
            self.set_default_size(800, 600)
            self.set_border_width(10)
            self.setup_ui()
        
        def setup_ui(self):
            vbox = Gtk.Box(orientation=Gtk.Orientation.VERTICAL, spacing=10)
            self.add(vbox)
            
            header = Gtk.Label()
            header.set_markup(f"<big><b>{APP_NAME}</b></big>")
            vbox.pack_start(header, False, False, 10)
            
            notebook = Gtk.Notebook()
            vbox.pack_start(notebook, True, True, 0)
            
            notebook.append_page(self.create_models_tab(), Gtk.Label(label="Models"))
            notebook.append_page(self.create_cache_tab(), Gtk.Label(label="Cache"))
        
        def create_models_tab(self):
            box = Gtk.Box(orientation=Gtk.Orientation.VERTICAL, spacing=10)
            box.set_margin_top(20)
            box.set_margin_start(20)
            
            for name, info in list(POPULAR_MODELS.items())[:6]:
                hbox = Gtk.Box(orientation=Gtk.Orientation.HORIZONTAL, spacing=10)
                label = Gtk.Label(label=f"{name}: {info['description']} ({info['size_gb']} GB)")
                label.set_xalign(0)
                hbox.pack_start(label, True, True, 10)
                
                btn = Gtk.Button(label="Download")
                btn.connect("clicked", self.on_download_model, name)
                btn.set_sensitive(self.app.license_tier >= LicenseTier.AIDEV)
                hbox.pack_end(btn, False, False, 10)
                
                box.pack_start(hbox, False, False, 5)
            
            return box
        
        def create_cache_tab(self):
            box = Gtk.Box(orientation=Gtk.Orientation.VERTICAL, spacing=10)
            box.set_margin_top(20)
            box.set_margin_start(20)
            
            cache = self.app.get_cache_size()
            
            label = Gtk.Label(label=f"Total Cache: {cache['total']} GB")
            box.pack_start(label, False, False, 10)
            
            return box
        
        def on_download_model(self, button, model_name):
            self.app.download_ollama_model(model_name)


def main():
    if not GTK_AVAILABLE:
        print("Cannot start Model Hub: GTK3 not available.", file=sys.stderr)
        sys.exit(1)
    parser = argparse.ArgumentParser(description=f"{APP_NAME}")
    parser.add_argument("--gui", action="store_true", help="Launch GUI mode")
    parser.add_argument("--cli", action="store_true", help="Run in CLI mode")
    parser.add_argument("--download", metavar="MODEL", help="Download model")
    parser.add_argument("--list", action="store_true", help="List installed models")
    parser.add_argument("--cache-size", action="store_true", help="Show cache size")
    parser.add_argument("--status", action="store_true", help="Show status")
    parser.add_argument("--version", action="version", version=f"{APP_NAME} {VERSION}")
    
    args = parser.parse_args()
    
    if args.download:
        app = AegisModelHub(headless=True)
        result = app.download_ollama_model(args.download)
        print(json.dumps(result, indent=2))
    elif args.list:
        app = AegisModelHub(headless=True)
        models = app.list_ollama_models()
        print(json.dumps(models, indent=2))
    elif args.cache_size:
        app = AegisModelHub(headless=True)
        print(json.dumps(app.get_cache_size(), indent=2))
    elif args.status:
        app = AegisModelHub(headless=True)
        print(json.dumps(app.get_status(), indent=2))
    elif args.cli or not GTK_AVAILABLE:
        app = AegisModelHub(headless=False)
        app.run_cli()
    else:
        app = AegisModelHub(headless=False)
        app.run_gui()


if __name__ == "__main__":
    main()
