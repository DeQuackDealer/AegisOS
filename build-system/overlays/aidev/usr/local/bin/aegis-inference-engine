#!/usr/bin/env python3
"""
Aegis Inference Engine - Local LLM inference with multi-backend support
Features: Ollama, llama.cpp, vLLM, ONNX Runtime with graceful fallback

Provides GUI (GTK) and CLI modes with tier-based feature gating.
"""

import os
import sys
import json
import subprocess
import logging
import argparse
import shutil
import time
import signal
import threading
import socket
from pathlib import Path
from typing import Dict, List, Optional, Any, Callable
from dataclasses import dataclass, field
from enum import Enum
from abc import ABC, abstractmethod

TIER_LIMIT = "aidev"
VERSION = "2.0.0"
APP_NAME = "Aegis Inference Engine"

CONFIG_FILE = "/etc/aegis/aidev-config.json"
LOG_FILE = "/var/log/aegis/inference-engine.log"
MODELS_DIR = "/var/lib/aegis/models"
CACHE_DIR = "/var/cache/aegis/inference"

try:
    import gi
    gi.require_version('Gtk', '3.0')
    from gi.repository import Gtk, GLib
    GTK_AVAILABLE = True
except ImportError:
    GTK_AVAILABLE = False
    print("Error: GTK3 is required. Install with: sudo pacman -S gtk3 python-gobject", file=sys.stderr)


class LicenseTier:
    FREEMIUM = 1
    AIDEV = 4


class ModelState(Enum):
    UNLOADED = "unloaded"
    LOADING = "loading"
    LOADED = "loaded"
    ERROR = "error"


@dataclass
class LoadedModel:
    name: str
    backend: str
    path: Optional[str] = None
    state: ModelState = ModelState.UNLOADED
    process: Optional[subprocess.Popen] = None
    port: Optional[int] = None
    memory_usage: int = 0


class InferenceBackend(ABC):
    name: str = "base"
    description: str = "Base backend"
    
    def __init__(self, config: Dict[str, Any], logger: logging.Logger):
        self.config = config
        self.logger = logger
        self.loaded_models: Dict[str, LoadedModel] = {}
    
    @abstractmethod
    def is_available(self) -> bool:
        pass
    
    @abstractmethod
    def run_inference(self, model: str, prompt: str, **kwargs) -> Dict[str, Any]:
        pass
    
    def load_model(self, model: str, **kwargs) -> Dict[str, Any]:
        return {"success": True, "message": f"Model {model} ready"}
    
    def unload_model(self, model: str) -> Dict[str, Any]:
        if model in self.loaded_models:
            loaded = self.loaded_models[model]
            if loaded.process:
                loaded.process.terminate()
                loaded.process.wait(timeout=10)
            del self.loaded_models[model]
            return {"success": True, "message": f"Model {model} unloaded"}
        return {"success": False, "error": f"Model {model} not loaded"}
    
    def get_loaded_models(self) -> List[str]:
        return list(self.loaded_models.keys())


class OllamaBackend(InferenceBackend):
    name = "ollama"
    description = "Easy model management and inference via Ollama"
    
    def __init__(self, config: Dict[str, Any], logger: logging.Logger):
        super().__init__(config, logger)
        self.server_process: Optional[subprocess.Popen] = None
        self.base_url = "http://localhost:11434"
    
    def is_available(self) -> bool:
        return shutil.which("ollama") is not None
    
    def _ensure_server_running(self) -> bool:
        if self._check_server():
            return True
        try:
            self.server_process = subprocess.Popen(
                ["ollama", "serve"],
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL,
                preexec_fn=os.setsid
            )
            for _ in range(30):
                time.sleep(0.5)
                if self._check_server():
                    return True
            return False
        except Exception as e:
            self.logger.error(f"Failed to start Ollama server: {e}")
            return False
    
    def _check_server(self) -> bool:
        try:
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.settimeout(1)
            result = sock.connect_ex(('localhost', 11434))
            sock.close()
            return result == 0
        except:
            return False
    
    def load_model(self, model: str, **kwargs) -> Dict[str, Any]:
        if not self._ensure_server_running():
            return {"success": False, "error": "Failed to start Ollama server"}
        
        self.loaded_models[model] = LoadedModel(
            name=model, backend=self.name, state=ModelState.LOADING
        )
        
        try:
            result = subprocess.run(
                ["ollama", "pull", model],
                capture_output=True, text=True, timeout=600
            )
            if result.returncode == 0:
                self.loaded_models[model].state = ModelState.LOADED
                return {"success": True, "message": f"Model {model} loaded"}
            else:
                self.loaded_models[model].state = ModelState.ERROR
                return {"success": False, "error": result.stderr}
        except subprocess.TimeoutExpired:
            self.loaded_models[model].state = ModelState.ERROR
            return {"success": False, "error": "Model pull timed out"}
        except Exception as e:
            self.loaded_models[model].state = ModelState.ERROR
            return {"success": False, "error": str(e)}
    
    def run_inference(self, model: str, prompt: str, **kwargs) -> Dict[str, Any]:
        if not self._ensure_server_running():
            return {"success": False, "error": "Ollama server not running"}
        
        timeout = kwargs.get("timeout", 120)
        temperature = kwargs.get("temperature", 0.7)
        max_tokens = kwargs.get("max_tokens", 256)
        
        try:
            env = os.environ.copy()
            cmd = ["ollama", "run", model]
            
            result = subprocess.run(
                cmd,
                input=prompt,
                capture_output=True,
                text=True,
                timeout=timeout,
                env=env
            )
            
            if result.returncode == 0:
                return {
                    "success": True,
                    "response": result.stdout.strip(),
                    "model": model,
                    "backend": self.name
                }
            else:
                return {"success": False, "error": result.stderr}
                
        except subprocess.TimeoutExpired:
            return {"success": False, "error": "Inference timed out"}
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def list_models(self) -> Dict[str, Any]:
        try:
            result = subprocess.run(
                ["ollama", "list"],
                capture_output=True, text=True, timeout=30
            )
            if result.returncode == 0:
                lines = result.stdout.strip().split('\n')
                models = []
                for line in lines[1:]:
                    parts = line.split()
                    if parts:
                        models.append({
                            "name": parts[0],
                            "size": parts[2] if len(parts) > 2 else "unknown"
                        })
                return {"success": True, "models": models}
            return {"success": False, "error": result.stderr}
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def stop_server(self):
        if self.server_process:
            try:
                os.killpg(os.getpgid(self.server_process.pid), signal.SIGTERM)
            except:
                self.server_process.terminate()
            self.server_process = None


class LlamaCppBackend(InferenceBackend):
    name = "llama.cpp"
    description = "CPU/GPU inference for GGUF models via llama.cpp"
    
    BINARY_NAMES = ["llama-cli", "llama-cpp", "main", "llama"]
    SERVER_NAMES = ["llama-server", "server"]
    
    def __init__(self, config: Dict[str, Any], logger: logging.Logger):
        super().__init__(config, logger)
        self.binary_path = self._find_binary()
        self.server_path = self._find_server()
        self.server_process: Optional[subprocess.Popen] = None
        self.server_port = 8080
    
    def _find_binary(self) -> Optional[str]:
        for name in self.BINARY_NAMES:
            path = shutil.which(name)
            if path:
                return path
        custom_paths = [
            "/usr/local/bin/llama.cpp/main",
            "/opt/llama.cpp/main",
            str(Path.home() / "llama.cpp" / "main"),
            str(Path.home() / ".local" / "bin" / "llama-cli"),
        ]
        for path in custom_paths:
            if os.path.isfile(path) and os.access(path, os.X_OK):
                return path
        return None
    
    def _find_server(self) -> Optional[str]:
        for name in self.SERVER_NAMES:
            path = shutil.which(name)
            if path:
                return path
        custom_paths = [
            "/usr/local/bin/llama.cpp/server",
            "/opt/llama.cpp/server",
            str(Path.home() / "llama.cpp" / "server"),
        ]
        for path in custom_paths:
            if os.path.isfile(path) and os.access(path, os.X_OK):
                return path
        return None
    
    def is_available(self) -> bool:
        return self.binary_path is not None
    
    def _find_model_file(self, model: str) -> Optional[str]:
        if os.path.isfile(model):
            return model
        
        search_paths = [
            MODELS_DIR,
            str(Path.home() / ".cache" / "llama.cpp"),
            str(Path.home() / "models"),
            "/usr/share/llama/models",
        ]
        
        for base_path in search_paths:
            for pattern in [model, f"{model}.gguf", f"*{model}*.gguf"]:
                path = Path(base_path)
                if path.exists():
                    matches = list(path.glob(pattern))
                    if matches:
                        return str(matches[0])
        return None
    
    def load_model(self, model: str, **kwargs) -> Dict[str, Any]:
        model_path = self._find_model_file(model)
        if not model_path:
            return {"success": False, "error": f"Model file not found: {model}"}
        
        self.loaded_models[model] = LoadedModel(
            name=model, backend=self.name, path=model_path, state=ModelState.LOADED
        )
        return {"success": True, "message": f"Model {model} loaded from {model_path}"}
    
    def run_inference(self, model: str, prompt: str, **kwargs) -> Dict[str, Any]:
        if not self.binary_path:
            return {"success": False, "error": "llama.cpp binary not found"}
        
        model_path = self._find_model_file(model)
        if not model_path:
            return {"success": False, "error": f"Model file not found: {model}"}
        
        threads = kwargs.get("threads", self.config.get("default_threads", 8))
        n_predict = kwargs.get("max_tokens", 256)
        ctx_size = kwargs.get("context_size", self.config.get("context_size", 4096))
        gpu_layers = kwargs.get("gpu_layers", self.config.get("gpu_layers", 0))
        temperature = kwargs.get("temperature", 0.7)
        timeout = kwargs.get("timeout", 120)
        
        cmd = [
            self.binary_path,
            "-m", model_path,
            "-p", prompt,
            "-n", str(n_predict),
            "-c", str(ctx_size),
            "-t", str(threads),
            "--temp", str(temperature),
        ]
        
        if gpu_layers > 0:
            cmd.extend(["-ngl", str(gpu_layers)])
        
        try:
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=timeout
            )
            
            if result.returncode == 0:
                output = result.stdout.strip()
                if prompt in output:
                    output = output.replace(prompt, "").strip()
                return {
                    "success": True,
                    "response": output,
                    "model": model,
                    "backend": self.name
                }
            else:
                return {"success": False, "error": result.stderr}
                
        except subprocess.TimeoutExpired:
            return {"success": False, "error": "Inference timed out"}
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def start_server(self, model: str, port: int = 8080) -> Dict[str, Any]:
        if not self.server_path:
            return {"success": False, "error": "llama.cpp server binary not found"}
        
        model_path = self._find_model_file(model)
        if not model_path:
            return {"success": False, "error": f"Model file not found: {model}"}
        
        ctx_size = self.config.get("context_size", 4096)
        threads = self.config.get("default_threads", 8)
        
        cmd = [
            self.server_path,
            "-m", model_path,
            "-c", str(ctx_size),
            "-t", str(threads),
            "--port", str(port),
            "--host", "0.0.0.0"
        ]
        
        try:
            self.server_process = subprocess.Popen(
                cmd,
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL,
                preexec_fn=os.setsid
            )
            self.server_port = port
            
            for _ in range(60):
                time.sleep(0.5)
                try:
                    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                    sock.settimeout(1)
                    if sock.connect_ex(('localhost', port)) == 0:
                        sock.close()
                        return {"success": True, "port": port}
                    sock.close()
                except:
                    pass
            
            return {"success": False, "error": "Server failed to start"}
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def stop_server(self):
        if self.server_process:
            try:
                os.killpg(os.getpgid(self.server_process.pid), signal.SIGTERM)
            except:
                self.server_process.terminate()
            self.server_process = None


class VLLMBackend(InferenceBackend):
    name = "vllm"
    description = "High-throughput inference server via vLLM"
    
    def __init__(self, config: Dict[str, Any], logger: logging.Logger):
        super().__init__(config, logger)
        self.server_process: Optional[subprocess.Popen] = None
        self.server_port = 8000
    
    def is_available(self) -> bool:
        try:
            result = subprocess.run(
                [sys.executable, "-c", "import vllm; print(vllm.__version__)"],
                capture_output=True, text=True, timeout=10
            )
            return result.returncode == 0
        except:
            return False
    
    def _check_server(self, port: int) -> bool:
        try:
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.settimeout(1)
            result = sock.connect_ex(('localhost', port))
            sock.close()
            return result == 0
        except:
            return False
    
    def load_model(self, model: str, **kwargs) -> Dict[str, Any]:
        port = kwargs.get("port", 8000)
        gpu_memory = kwargs.get("gpu_memory_utilization", 0.9)
        tensor_parallel = kwargs.get("tensor_parallel_size", 1)
        
        cmd = [
            sys.executable, "-m", "vllm.entrypoints.openai.api_server",
            "--model", model,
            "--port", str(port),
            "--gpu-memory-utilization", str(gpu_memory),
            "--tensor-parallel-size", str(tensor_parallel),
        ]
        
        try:
            self.server_process = subprocess.Popen(
                cmd,
                stdout=subprocess.DEVNULL,
                stderr=subprocess.PIPE,
                preexec_fn=os.setsid
            )
            self.server_port = port
            
            for i in range(120):
                time.sleep(1)
                if self._check_server(port):
                    self.loaded_models[model] = LoadedModel(
                        name=model, backend=self.name, state=ModelState.LOADED,
                        process=self.server_process, port=port
                    )
                    return {"success": True, "message": f"vLLM server started on port {port}"}
                if self.server_process.poll() is not None:
                    stderr = self.server_process.stderr.read().decode() if self.server_process.stderr else ""
                    return {"success": False, "error": f"Server exited: {stderr}"}
            
            return {"success": False, "error": "vLLM server failed to start within timeout"}
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def run_inference(self, model: str, prompt: str, **kwargs) -> Dict[str, Any]:
        port = kwargs.get("port", self.server_port)
        max_tokens = kwargs.get("max_tokens", 256)
        temperature = kwargs.get("temperature", 0.7)
        
        if not self._check_server(port):
            return {"success": False, "error": "vLLM server not running"}
        
        try:
            import urllib.request
            import urllib.error
            
            url = f"http://localhost:{port}/v1/completions"
            data = json.dumps({
                "model": model,
                "prompt": prompt,
                "max_tokens": max_tokens,
                "temperature": temperature
            }).encode()
            
            req = urllib.request.Request(
                url,
                data=data,
                headers={"Content-Type": "application/json"}
            )
            
            with urllib.request.urlopen(req, timeout=kwargs.get("timeout", 120)) as response:
                result = json.loads(response.read().decode())
                if "choices" in result and result["choices"]:
                    return {
                        "success": True,
                        "response": result["choices"][0].get("text", ""),
                        "model": model,
                        "backend": self.name
                    }
                return {"success": False, "error": "No response from vLLM"}
                
        except urllib.error.URLError as e:
            return {"success": False, "error": f"Connection error: {e}"}
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def unload_model(self, model: str) -> Dict[str, Any]:
        if self.server_process:
            try:
                os.killpg(os.getpgid(self.server_process.pid), signal.SIGTERM)
                self.server_process.wait(timeout=30)
            except:
                self.server_process.kill()
            self.server_process = None
        return super().unload_model(model)


class ONNXBackend(InferenceBackend):
    name = "onnxruntime"
    description = "ONNX model inference for optimized models"
    
    def __init__(self, config: Dict[str, Any], logger: logging.Logger):
        super().__init__(config, logger)
        self.sessions: Dict[str, Any] = {}
        self._ort = None
    
    def is_available(self) -> bool:
        try:
            import onnxruntime
            self._ort = onnxruntime
            return True
        except ImportError:
            return False
    
    def _find_model_file(self, model: str) -> Optional[str]:
        if os.path.isfile(model):
            return model
        
        search_paths = [
            MODELS_DIR,
            str(Path.home() / ".cache" / "onnx"),
            str(Path.home() / "models"),
            "/usr/share/onnx/models",
        ]
        
        for base_path in search_paths:
            for pattern in [model, f"{model}.onnx", f"*{model}*.onnx"]:
                path = Path(base_path)
                if path.exists():
                    matches = list(path.glob(pattern))
                    if matches:
                        return str(matches[0])
        return None
    
    def load_model(self, model: str, **kwargs) -> Dict[str, Any]:
        if not self._ort:
            try:
                import onnxruntime
                self._ort = onnxruntime
            except ImportError:
                return {"success": False, "error": "ONNX Runtime not installed"}
        
        model_path = self._find_model_file(model)
        if not model_path:
            return {"success": False, "error": f"ONNX model file not found: {model}"}
        
        try:
            providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']
            available_providers = self._ort.get_available_providers()
            providers = [p for p in providers if p in available_providers]
            
            sess_options = self._ort.SessionOptions()
            sess_options.graph_optimization_level = self._ort.GraphOptimizationLevel.ORT_ENABLE_ALL
            
            num_threads = kwargs.get("threads", self.config.get("default_threads", 8))
            sess_options.intra_op_num_threads = num_threads
            sess_options.inter_op_num_threads = num_threads
            
            session = self._ort.InferenceSession(
                model_path,
                sess_options=sess_options,
                providers=providers
            )
            
            self.sessions[model] = session
            self.loaded_models[model] = LoadedModel(
                name=model, backend=self.name, path=model_path, state=ModelState.LOADED
            )
            
            return {
                "success": True,
                "message": f"ONNX model {model} loaded",
                "providers": [p for p in providers if p in available_providers]
            }
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def run_inference(self, model: str, prompt: str, **kwargs) -> Dict[str, Any]:
        if model not in self.sessions:
            load_result = self.load_model(model)
            if not load_result["success"]:
                return load_result
        
        session = self.sessions.get(model)
        if not session:
            return {"success": False, "error": "Model session not available"}
        
        try:
            import numpy as np
            
            input_name = session.get_inputs()[0].name
            input_shape = session.get_inputs()[0].shape
            
            tokenizer = kwargs.get("tokenizer")
            if tokenizer:
                input_ids = tokenizer.encode(prompt, return_tensors="np")
            else:
                input_ids = np.array([[ord(c) for c in prompt[:input_shape[1] if len(input_shape) > 1 else 512]]], dtype=np.int64)
            
            outputs = session.run(None, {input_name: input_ids})
            
            if tokenizer:
                response = tokenizer.decode(outputs[0][0], skip_special_tokens=True)
            else:
                if outputs[0].dtype in [np.float32, np.float64]:
                    response = f"Model output shape: {outputs[0].shape}, values: {outputs[0].flatten()[:10].tolist()}..."
                else:
                    response = ''.join([chr(min(max(int(x), 32), 126)) for x in outputs[0].flatten()[:256]])
            
            return {
                "success": True,
                "response": response,
                "model": model,
                "backend": self.name,
                "output_shape": str(outputs[0].shape)
            }
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def unload_model(self, model: str) -> Dict[str, Any]:
        if model in self.sessions:
            del self.sessions[model]
        return super().unload_model(model)
    
    def get_model_info(self, model: str) -> Dict[str, Any]:
        if model not in self.sessions:
            load_result = self.load_model(model)
            if not load_result["success"]:
                return load_result
        
        session = self.sessions.get(model)
        if not session:
            return {"success": False, "error": "Session not available"}
        
        try:
            inputs = [{"name": i.name, "shape": str(i.shape), "type": i.type} for i in session.get_inputs()]
            outputs = [{"name": o.name, "shape": str(o.shape), "type": o.type} for o in session.get_outputs()]
            
            return {
                "success": True,
                "inputs": inputs,
                "outputs": outputs,
                "providers": session.get_providers()
            }
        except Exception as e:
            return {"success": False, "error": str(e)}


class AegisInferenceEngine:
    def __init__(self, headless: bool = False):
        self.headless = headless
        self.version = VERSION
        self.config = {}
        self.license_tier = LicenseTier.FREEMIUM
        self.backends: Dict[str, InferenceBackend] = {}
        
        self.setup_logging()
        self.load_license_tier()
        self.load_config()
        self.initialize_backends()
        
    def setup_logging(self):
        log_dir = Path(LOG_FILE).parent
        try:
            if not log_dir.exists():
                log_dir.mkdir(parents=True, exist_ok=True)
        except (PermissionError, OSError):
            pass
        
        try:
            handlers = []
            try:
                if log_dir.exists() and os.access(str(log_dir), os.W_OK):
                    handlers.append(logging.FileHandler(LOG_FILE))
            except (PermissionError, OSError):
                pass
            if not self.headless:
                handlers.append(logging.StreamHandler())
            if not handlers:
                handlers.append(logging.NullHandler())
            
            logging.basicConfig(
                level=logging.INFO,
                format='%(asctime)s - %(levelname)s - [%(name)s] %(message)s',
                handlers=handlers,
                force=True
            )
        except Exception:
            logging.basicConfig(level=logging.INFO, handlers=[logging.StreamHandler()], force=True)
        
        self.logger = logging.getLogger("AegisInferenceEngine")
    
    def load_license_tier(self):
        license_file = Path("/etc/aegis/license.json")
        try:
            if license_file.exists():
                with open(license_file, 'r') as f:
                    license_data = json.load(f)
                edition = license_data.get('edition', 'freemium').lower()
                if edition == 'aidev':
                    self.license_tier = LicenseTier.AIDEV
            elif Path("/etc/aegis-aidev-marker").exists():
                self.license_tier = LicenseTier.AIDEV
        except Exception:
            pass
    
    def is_feature_available(self) -> bool:
        return self.license_tier >= LicenseTier.AIDEV
    
    def load_config(self):
        default_config = {
            "default_threads": 8,
            "gpu_layers": 35,
            "context_size": 4096,
            "default_backend": "ollama",
            "fallback_order": ["ollama", "llama.cpp", "vllm", "onnxruntime"],
            "timeout": 120,
            "temperature": 0.7,
            "max_tokens": 256
        }
        
        try:
            if Path(CONFIG_FILE).exists():
                with open(CONFIG_FILE, 'r') as f:
                    file_config = json.load(f)
                    if "features" in file_config and "inference_engine" in file_config["features"]:
                        self.config = {**default_config, **file_config["features"]["inference_engine"]}
                    else:
                        self.config = default_config
            else:
                self.config = default_config
        except Exception:
            self.config = default_config
    
    def initialize_backends(self):
        backend_classes = [
            OllamaBackend,
            LlamaCppBackend,
            VLLMBackend,
            ONNXBackend
        ]
        
        for cls in backend_classes:
            try:
                backend = cls(self.config, self.logger)
                self.backends[backend.name] = backend
                self.logger.info(f"Backend {backend.name} initialized (available: {backend.is_available()})")
            except Exception as e:
                self.logger.warning(f"Failed to initialize backend {cls.name}: {e}")
    
    def get_available_backends(self) -> List[Dict[str, Any]]:
        result = []
        for name, backend in self.backends.items():
            result.append({
                "name": name,
                "description": backend.description,
                "available": backend.is_available(),
                "loaded_models": backend.get_loaded_models()
            })
        return result
    
    def _get_backend(self, backend_name: Optional[str] = None) -> Optional[InferenceBackend]:
        if backend_name and backend_name in self.backends:
            backend = self.backends[backend_name]
            if backend.is_available():
                return backend
        
        fallback_order = self.config.get("fallback_order", ["ollama", "llama.cpp", "vllm", "onnxruntime"])
        for name in fallback_order:
            if name in self.backends and self.backends[name].is_available():
                self.logger.info(f"Using fallback backend: {name}")
                return self.backends[name]
        
        return None
    
    def load_model(self, model: str, backend: Optional[str] = None, **kwargs) -> Dict[str, Any]:
        if not self.is_feature_available():
            return {"success": False, "error": "Model loading requires AI Developer edition"}
        
        backend_instance = self._get_backend(backend)
        if not backend_instance:
            return {"success": False, "error": "No available backend found"}
        
        self.logger.info(f"Loading model {model} with backend {backend_instance.name}")
        return backend_instance.load_model(model, **kwargs)
    
    def unload_model(self, model: str, backend: Optional[str] = None) -> Dict[str, Any]:
        if backend and backend in self.backends:
            return self.backends[backend].unload_model(model)
        
        for name, b in self.backends.items():
            if model in b.get_loaded_models():
                return b.unload_model(model)
        
        return {"success": False, "error": f"Model {model} not found in any backend"}
    
    def run_inference(self, model: str, prompt: str, backend: Optional[str] = None, **kwargs) -> Dict[str, Any]:
        if not self.is_feature_available():
            return {"success": False, "error": "Inference requires AI Developer edition"}
        
        backend_instance = self._get_backend(backend)
        if not backend_instance:
            return {"success": False, "error": "No available backend found"}
        
        merged_kwargs = {**self.config, **kwargs}
        
        self.logger.info(f"Running inference with model {model} on backend {backend_instance.name}")
        result = backend_instance.run_inference(model, prompt, **merged_kwargs)
        
        if not result.get("success") and not backend:
            self.logger.warning(f"Backend {backend_instance.name} failed, trying fallback...")
            fallback_order = self.config.get("fallback_order", [])
            current_idx = fallback_order.index(backend_instance.name) if backend_instance.name in fallback_order else -1
            
            for name in fallback_order[current_idx + 1:]:
                if name in self.backends and self.backends[name].is_available():
                    self.logger.info(f"Trying fallback backend: {name}")
                    result = self.backends[name].run_inference(model, prompt, **merged_kwargs)
                    if result.get("success"):
                        break
        
        return result
    
    def start_server(self, model: str = "llama3.2", port: int = 11434, backend: Optional[str] = None) -> Dict[str, Any]:
        if not self.is_feature_available():
            return {"success": False, "error": "Server requires AI Developer edition"}
        
        backend_instance = self._get_backend(backend)
        if not backend_instance:
            return {"success": False, "error": "No available backend found"}
        
        if hasattr(backend_instance, 'start_server'):
            return backend_instance.start_server(model, port)
        elif hasattr(backend_instance, 'load_model'):
            return backend_instance.load_model(model, port=port)
        
        return {"success": False, "error": "Backend does not support server mode"}
    
    def stop_server(self, backend: Optional[str] = None) -> bool:
        if backend and backend in self.backends:
            b = self.backends[backend]
            if hasattr(b, 'stop_server'):
                b.stop_server()
                return True
        
        for name, b in self.backends.items():
            if hasattr(b, 'stop_server'):
                b.stop_server()
        return True
    
    def get_status(self) -> Dict[str, Any]:
        all_loaded = {}
        for name, backend in self.backends.items():
            models = backend.get_loaded_models()
            if models:
                all_loaded[name] = models
        
        return {
            "version": self.version,
            "backends": self.get_available_backends(),
            "config": self.config,
            "tier": self.license_tier,
            "loaded_models": all_loaded
        }
    
    def list_models(self, backend: Optional[str] = None) -> Dict[str, Any]:
        if backend == "ollama" and "ollama" in self.backends:
            ollama = self.backends["ollama"]
            if isinstance(ollama, OllamaBackend):
                return ollama.list_models()
        
        return {"success": True, "models": [], "message": "Use --backend ollama for model listing"}
    
    def cleanup(self):
        for name, backend in self.backends.items():
            try:
                if hasattr(backend, 'stop_server'):
                    backend.stop_server()
                for model in list(backend.get_loaded_models()):
                    backend.unload_model(model)
            except Exception as e:
                self.logger.warning(f"Cleanup error for backend {name}: {e}")
    
    def run_server_mode(self, model: str = "llama3.2", port: int = 11434, backend: Optional[str] = None):
        result = self.start_server(model, port, backend)
        if result["success"]:
            print(f"Inference server running on port {result.get('port', port)}")
            print("Press Ctrl+C to stop...")
            try:
                while True:
                    time.sleep(1)
            except KeyboardInterrupt:
                print("\nShutting down...")
                self.cleanup()
        else:
            print(f"Failed to start server: {result.get('error')}")
            sys.exit(1)
    
    def run_gui(self):
        if not GTK_AVAILABLE:
            return self.run_cli()
        
        win = InferenceWindow(self)
        win.connect("destroy", lambda w: (self.cleanup(), Gtk.main_quit()))
        win.show_all()
        Gtk.main()
    
    def run_cli(self):
        print(f"\n{'='*60}")
        print(f"  {APP_NAME} v{VERSION}")
        print(f"{'='*60}\n")
        
        status = self.get_status()
        
        print("Inference Backends:")
        for backend in status["backends"]:
            available = "✓" if backend["available"] else "✗"
            models = f" (loaded: {', '.join(backend['loaded_models'])})" if backend['loaded_models'] else ""
            print(f"  {backend['name']}: {available} - {backend['description']}{models}")
        
        print(f"\nConfiguration:")
        print(f"  Default Backend: {self.config.get('default_backend')}")
        print(f"  Threads: {self.config.get('default_threads')}")
        print(f"  GPU Layers: {self.config.get('gpu_layers')}")
        print(f"  Context Size: {self.config.get('context_size')}")
        print(f"  Fallback Order: {' -> '.join(self.config.get('fallback_order', []))}")
        
        if not self.is_feature_available():
            print(f"\n⚠ AI Developer edition required for inference features")


if GTK_AVAILABLE:
    class InferenceWindow(Gtk.Window):
        def __init__(self, app: AegisInferenceEngine):
            super().__init__(title=f"{APP_NAME} v{VERSION}")
            self.app = app
            self.set_default_size(900, 700)
            self.set_border_width(10)
            self.setup_ui()
        
        def setup_ui(self):
            vbox = Gtk.Box(orientation=Gtk.Orientation.VERTICAL, spacing=10)
            self.add(vbox)
            
            header = Gtk.Label()
            header.set_markup(f"<big><b>{APP_NAME}</b></big>")
            vbox.pack_start(header, False, False, 10)
            
            frame = Gtk.Frame(label="Available Backends")
            vbox.pack_start(frame, False, False, 5)
            
            backends_box = Gtk.Box(orientation=Gtk.Orientation.VERTICAL, spacing=5)
            backends_box.set_margin_start(10)
            backends_box.set_margin_end(10)
            backends_box.set_margin_top(5)
            backends_box.set_margin_bottom(5)
            frame.add(backends_box)
            
            for backend in self.app.get_available_backends():
                hbox = Gtk.Box(orientation=Gtk.Orientation.HORIZONTAL, spacing=10)
                status = "✓" if backend["available"] else "✗"
                color = "green" if backend["available"] else "red"
                label = Gtk.Label()
                label.set_markup(f"<span foreground='{color}'>{status}</span> <b>{backend['name']}</b>: {backend['description']}")
                label.set_xalign(0)
                hbox.pack_start(label, True, True, 0)
                backends_box.pack_start(hbox, False, False, 2)
            
            prompt_frame = Gtk.Frame(label="Inference")
            vbox.pack_start(prompt_frame, True, True, 5)
            
            prompt_box = Gtk.Box(orientation=Gtk.Orientation.VERTICAL, spacing=5)
            prompt_box.set_margin_start(10)
            prompt_box.set_margin_end(10)
            prompt_box.set_margin_top(5)
            prompt_box.set_margin_bottom(5)
            prompt_frame.add(prompt_box)
            
            model_box = Gtk.Box(orientation=Gtk.Orientation.HORIZONTAL, spacing=10)
            model_box.pack_start(Gtk.Label(label="Model:"), False, False, 0)
            self.model_entry = Gtk.Entry()
            self.model_entry.set_text("llama3.2")
            self.model_entry.set_hexpand(True)
            model_box.pack_start(self.model_entry, True, True, 0)
            prompt_box.pack_start(model_box, False, False, 0)
            
            prompt_box.pack_start(Gtk.Label(label="Prompt:", xalign=0), False, False, 0)
            
            scroll = Gtk.ScrolledWindow()
            scroll.set_min_content_height(100)
            self.prompt_text = Gtk.TextView()
            self.prompt_text.set_wrap_mode(Gtk.WrapMode.WORD)
            scroll.add(self.prompt_text)
            prompt_box.pack_start(scroll, False, True, 0)
            
            self.run_button = Gtk.Button(label="Run Inference")
            self.run_button.connect("clicked", self.on_run_clicked)
            prompt_box.pack_start(self.run_button, False, False, 5)
            
            prompt_box.pack_start(Gtk.Label(label="Response:", xalign=0), False, False, 0)
            
            response_scroll = Gtk.ScrolledWindow()
            response_scroll.set_min_content_height(200)
            self.response_text = Gtk.TextView()
            self.response_text.set_wrap_mode(Gtk.WrapMode.WORD)
            self.response_text.set_editable(False)
            response_scroll.add(self.response_text)
            prompt_box.pack_start(response_scroll, True, True, 0)
            
            self.status_bar = Gtk.Statusbar()
            vbox.pack_start(self.status_bar, False, False, 0)
            self.status_bar.push(0, "Ready")
        
        def on_run_clicked(self, button):
            model = self.model_entry.get_text().strip()
            buffer = self.prompt_text.get_buffer()
            start, end = buffer.get_bounds()
            prompt = buffer.get_text(start, end, False).strip()
            
            if not model or not prompt:
                self.status_bar.push(0, "Please enter model and prompt")
                return
            
            self.run_button.set_sensitive(False)
            self.status_bar.push(0, "Running inference...")
            
            def run_async():
                result = self.app.run_inference(model, prompt)
                GLib.idle_add(self.show_result, result)
            
            thread = threading.Thread(target=run_async)
            thread.daemon = True
            thread.start()
        
        def show_result(self, result):
            self.run_button.set_sensitive(True)
            buffer = self.response_text.get_buffer()
            
            if result.get("success"):
                buffer.set_text(result.get("response", ""))
                self.status_bar.push(0, f"Completed using {result.get('backend', 'unknown')}")
            else:
                buffer.set_text(f"Error: {result.get('error', 'Unknown error')}")
                self.status_bar.push(0, "Inference failed")


def main():
    if not GTK_AVAILABLE:
        print("Cannot start Inference Engine: GTK3 not available.", file=sys.stderr)
        sys.exit(1)
    parser = argparse.ArgumentParser(description=f"{APP_NAME}")
    parser.add_argument("--gui", action="store_true", help="Launch GUI mode")
    parser.add_argument("--cli", action="store_true", help="Run in CLI mode")
    parser.add_argument("--server", action="store_true", help="Run inference server")
    parser.add_argument("--run", nargs=2, metavar=("MODEL", "PROMPT"), help="Run inference")
    parser.add_argument("--load", metavar="MODEL", help="Load a model")
    parser.add_argument("--unload", metavar="MODEL", help="Unload a model")
    parser.add_argument("--list-models", action="store_true", help="List available models")
    parser.add_argument("--backend", metavar="NAME", help="Specify backend (ollama, llama.cpp, vllm, onnxruntime)")
    parser.add_argument("--port", type=int, default=11434, help="Server port (default: 11434)")
    parser.add_argument("--status", action="store_true", help="Show status")
    parser.add_argument("--version", action="version", version=f"{APP_NAME} {VERSION}")
    
    args = parser.parse_args()
    
    if args.server:
        app = AegisInferenceEngine(headless=True)
        app.run_server_mode(port=args.port, backend=args.backend)
    elif args.run:
        app = AegisInferenceEngine(headless=True)
        result = app.run_inference(args.run[0], args.run[1], backend=args.backend)
        print(json.dumps(result, indent=2))
        app.cleanup()
    elif args.load:
        app = AegisInferenceEngine(headless=True)
        result = app.load_model(args.load, backend=args.backend)
        print(json.dumps(result, indent=2))
    elif args.unload:
        app = AegisInferenceEngine(headless=True)
        result = app.unload_model(args.unload, backend=args.backend)
        print(json.dumps(result, indent=2))
    elif args.list_models:
        app = AegisInferenceEngine(headless=True)
        result = app.list_models(backend=args.backend or "ollama")
        print(json.dumps(result, indent=2))
    elif args.status:
        app = AegisInferenceEngine(headless=True)
        print(json.dumps(app.get_status(), indent=2))
    elif args.cli or not GTK_AVAILABLE:
        app = AegisInferenceEngine(headless=False)
        app.run_cli()
    else:
        app = AegisInferenceEngine(headless=False)
        app.run_gui()


if __name__ == "__main__":
    main()
