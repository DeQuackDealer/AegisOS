#!/usr/bin/env python3
"""
Aegis Training Optimizer - Training optimization service
Features: Batch size tuning, VRAM planning, mixed precision, checkpointing

Provides GUI (GTK) and CLI modes with tier-based feature gating.
"""

import os
import sys
import json
import subprocess
import logging
import argparse
import shutil
import time
import threading
from pathlib import Path
from typing import Dict, List, Optional, Any, Callable
from dataclasses import dataclass, asdict
from enum import Enum

TIER_LIMIT = "aidev"
VERSION = "1.5.0"
APP_NAME = "Aegis Training Optimizer"

CONFIG_FILE = "/etc/aegis/aidev-config.json"
LOG_FILE = "/var/log/aegis/training-optimizer.log"
CHECKPOINT_DIR = "/var/lib/aegis/checkpoints"

try:
    import gi
    gi.require_version('Gtk', '3.0')
    from gi.repository import Gtk, GLib
    GTK_AVAILABLE = True
except ImportError:
    GTK_AVAILABLE = False
    print("Error: GTK3 is required. Install with: sudo pacman -S gtk3 python-gobject", file=sys.stderr)


class LicenseTier:
    FREEMIUM = 1
    BASIC = 2
    AIDEV = 4
    SERVER = 5


class PrecisionMode(Enum):
    FP32 = "fp32"
    FP16 = "fp16"
    BF16 = "bf16"
    INT8 = "int8"
    MIXED = "mixed"


@dataclass
class VRAMAllocation:
    model_memory_mb: int
    optimizer_memory_mb: int
    gradient_memory_mb: int
    activation_memory_mb: int
    buffer_memory_mb: int
    total_required_mb: int
    available_mb: int
    utilization_percent: float


@dataclass
class BatchSizeRecommendation:
    recommended_batch_size: int
    max_batch_size: int
    memory_per_sample_mb: float
    gradient_accumulation_steps: int
    effective_batch_size: int


@dataclass
class TrainingConfig:
    batch_size: int
    gradient_accumulation: int
    precision: str
    checkpoint_interval: int
    async_data_loading: bool
    num_workers: int
    prefetch_factor: int
    pin_memory: bool


@dataclass
class CheckpointInfo:
    path: str
    epoch: int
    step: int
    timestamp: str
    size_mb: float
    model_name: str


class AegisTrainingOptimizer:
    def __init__(self, headless: bool = False):
        self.headless = headless
        self.version = VERSION
        self.config = {}
        self.license_tier = LicenseTier.FREEMIUM
        self.active_training = False
        self.checkpoint_thread = None
        
        self.setup_logging()
        self.load_license_tier()
        self.load_config()
        self.ensure_checkpoint_dir()
        
    def setup_logging(self):
        log_dir = Path(LOG_FILE).parent
        try:
            log_dir.mkdir(parents=True, exist_ok=True)
        except (PermissionError, OSError):
            pass
        
        try:
            logging.basicConfig(
                level=logging.INFO,
                format='%(asctime)s - %(levelname)s - [%(name)s] %(message)s',
                handlers=[
                    logging.FileHandler(LOG_FILE) if os.access(str(log_dir), os.W_OK) else logging.NullHandler(),
                    logging.StreamHandler() if not self.headless else logging.NullHandler()
                ]
            )
        except Exception:
            logging.basicConfig(level=logging.INFO, handlers=[logging.StreamHandler()])
        
        self.logger = logging.getLogger("AegisTrainingOptimizer")
        self.logger.info(f"Starting {APP_NAME} v{VERSION}")
    
    def load_license_tier(self):
        license_file = Path("/etc/aegis/license.json")
        try:
            if license_file.exists():
                with open(license_file, 'r') as f:
                    license_data = json.load(f)
                edition = license_data.get('edition', 'freemium').lower()
                tier_map = {
                    'freemium': LicenseTier.FREEMIUM,
                    'basic': LicenseTier.BASIC,
                    'aidev': LicenseTier.AIDEV,
                    'server': LicenseTier.SERVER
                }
                self.license_tier = tier_map.get(edition, LicenseTier.FREEMIUM)
            else:
                if Path("/etc/aegis-aidev-marker").exists():
                    self.license_tier = LicenseTier.AIDEV
        except Exception as e:
            self.logger.warning(f"Failed to load license tier: {e}")
    
    def is_feature_available(self) -> bool:
        return self.license_tier >= LicenseTier.AIDEV
    
    def load_config(self):
        default_config = {
            "default_precision": "fp16",
            "checkpoint_interval_minutes": 30,
            "max_checkpoints": 5,
            "auto_batch_size": True,
            "vram_buffer_percent": 10,
            "async_data_loading": True,
            "num_workers": 4,
            "prefetch_factor": 2,
            "gradient_accumulation": 1
        }
        
        try:
            if Path(CONFIG_FILE).exists():
                with open(CONFIG_FILE, 'r') as f:
                    file_config = json.load(f)
                    if "features" in file_config and "training_optimizer" in file_config["features"]:
                        self.config = {**default_config, **file_config["features"]["training_optimizer"]}
                    else:
                        self.config = default_config
            else:
                self.config = default_config
        except Exception as e:
            self.logger.error(f"Error loading config: {e}")
            self.config = default_config
    
    def ensure_checkpoint_dir(self):
        try:
            Path(CHECKPOINT_DIR).mkdir(parents=True, exist_ok=True)
        except (PermissionError, OSError):
            pass

    def get_gpu_memory(self) -> Dict[str, int]:
        memory_info = {"total_mb": 0, "free_mb": 0, "used_mb": 0}
        
        if shutil.which("nvidia-smi"):
            try:
                result = subprocess.run(
                    ["nvidia-smi", "--query-gpu=memory.total,memory.free,memory.used",
                     "--format=csv,noheader,nounits"],
                    capture_output=True, text=True, timeout=10
                )
                if result.returncode == 0:
                    total, free, used = 0, 0, 0
                    for line in result.stdout.strip().split('\n'):
                        parts = [int(p.strip()) for p in line.split(',')]
                        total += parts[0]
                        free += parts[1]
                        used += parts[2]
                    memory_info = {"total_mb": total, "free_mb": free, "used_mb": used}
            except Exception as e:
                self.logger.error(f"Error getting GPU memory: {e}")
        
        return memory_info

    def calculate_vram_allocation(self, model_params_millions: float, 
                                   precision: str = "fp16") -> VRAMAllocation:
        bytes_per_param = {
            "fp32": 4, "fp16": 2, "bf16": 2, "int8": 1, "mixed": 2.5
        }
        
        bpp = bytes_per_param.get(precision, 2)
        params = model_params_millions * 1e6
        
        model_memory = int((params * bpp) / (1024 * 1024))
        optimizer_memory = int(model_memory * 2)
        gradient_memory = model_memory
        activation_memory = int(model_memory * 0.5)
        buffer_memory = int((model_memory + optimizer_memory) * 0.1)
        
        total_required = model_memory + optimizer_memory + gradient_memory + activation_memory + buffer_memory
        
        gpu_mem = self.get_gpu_memory()
        available = gpu_mem.get("free_mb", 0)
        utilization = (total_required / available * 100) if available > 0 else 100
        
        return VRAMAllocation(
            model_memory_mb=model_memory,
            optimizer_memory_mb=optimizer_memory,
            gradient_memory_mb=gradient_memory,
            activation_memory_mb=activation_memory,
            buffer_memory_mb=buffer_memory,
            total_required_mb=total_required,
            available_mb=available,
            utilization_percent=min(utilization, 100)
        )

    def recommend_batch_size(self, model_params_millions: float,
                              sequence_length: int = 512,
                              precision: str = "fp16") -> BatchSizeRecommendation:
        if not self.is_feature_available():
            return BatchSizeRecommendation(
                recommended_batch_size=1,
                max_batch_size=1,
                memory_per_sample_mb=0,
                gradient_accumulation_steps=1,
                effective_batch_size=1
            )
        
        gpu_mem = self.get_gpu_memory()
        available_mb = gpu_mem.get("free_mb", 8192)
        
        buffer_percent = self.config.get("vram_buffer_percent", 10)
        usable_mb = available_mb * (1 - buffer_percent / 100)
        
        vram = self.calculate_vram_allocation(model_params_millions, precision)
        overhead_mb = vram.model_memory_mb + vram.optimizer_memory_mb + vram.buffer_memory_mb
        
        remaining_mb = usable_mb - overhead_mb
        
        bytes_per_param = {"fp32": 4, "fp16": 2, "bf16": 2, "int8": 1, "mixed": 2.5}
        bpp = bytes_per_param.get(precision, 2)
        
        memory_per_sample = (sequence_length * 768 * bpp * 2) / (1024 * 1024)
        
        max_batch = int(remaining_mb / memory_per_sample) if memory_per_sample > 0 else 1
        max_batch = max(1, max_batch)
        
        recommended = max(1, int(max_batch * 0.8))
        
        target_effective = 32
        grad_accum = max(1, target_effective // recommended)
        effective = recommended * grad_accum
        
        return BatchSizeRecommendation(
            recommended_batch_size=recommended,
            max_batch_size=max_batch,
            memory_per_sample_mb=memory_per_sample,
            gradient_accumulation_steps=grad_accum,
            effective_batch_size=effective
        )

    def get_optimal_training_config(self, model_params_millions: float,
                                     dataset_size: int = 10000) -> TrainingConfig:
        if not self.is_feature_available():
            return TrainingConfig(
                batch_size=1,
                gradient_accumulation=1,
                precision="fp32",
                checkpoint_interval=1000,
                async_data_loading=False,
                num_workers=0,
                prefetch_factor=2,
                pin_memory=False
            )
        
        batch_rec = self.recommend_batch_size(model_params_millions)
        
        gpu_mem = self.get_gpu_memory()
        has_tensor_cores = gpu_mem.get("total_mb", 0) > 8000
        precision = "bf16" if has_tensor_cores else "fp16"
        
        steps_per_epoch = dataset_size // batch_rec.effective_batch_size
        checkpoint_interval = max(100, steps_per_epoch // 5)
        
        cpu_count = os.cpu_count() or 4
        num_workers = min(cpu_count - 1, 8)
        
        return TrainingConfig(
            batch_size=batch_rec.recommended_batch_size,
            gradient_accumulation=batch_rec.gradient_accumulation_steps,
            precision=precision,
            checkpoint_interval=checkpoint_interval,
            async_data_loading=True,
            num_workers=num_workers,
            prefetch_factor=2,
            pin_memory=True
        )

    def configure_async_pipeline(self) -> Dict[str, Any]:
        if not self.is_feature_available():
            return {"success": False, "error": "Async pipeline requires AI Developer edition"}
        
        cpu_count = os.cpu_count() or 4
        
        config = {
            "num_workers": min(cpu_count - 1, 8),
            "prefetch_factor": 2,
            "pin_memory": True,
            "persistent_workers": True,
            "non_blocking_transfer": True,
            "cuda_streams": 2
        }
        
        self.logger.info(f"Async pipeline configured: {config}")
        return {"success": True, "config": config}

    def get_precision_config(self, mode: str) -> Dict[str, Any]:
        configs = {
            "fp32": {
                "dtype": "float32",
                "autocast": False,
                "grad_scaler": False,
                "memory_efficient": False
            },
            "fp16": {
                "dtype": "float16",
                "autocast": True,
                "grad_scaler": True,
                "memory_efficient": True
            },
            "bf16": {
                "dtype": "bfloat16",
                "autocast": True,
                "grad_scaler": False,
                "memory_efficient": True
            },
            "int8": {
                "dtype": "int8",
                "autocast": False,
                "grad_scaler": False,
                "memory_efficient": True,
                "quantization": True
            },
            "mixed": {
                "dtype": "mixed",
                "autocast": True,
                "grad_scaler": True,
                "memory_efficient": True
            }
        }
        
        return configs.get(mode, configs["fp16"])

    def list_checkpoints(self, model_name: Optional[str] = None) -> List[CheckpointInfo]:
        checkpoints = []
        
        try:
            checkpoint_path = Path(CHECKPOINT_DIR)
            if not checkpoint_path.exists():
                return checkpoints
            
            for ckpt_file in checkpoint_path.glob("*.ckpt"):
                try:
                    stat = ckpt_file.stat()
                    name_parts = ckpt_file.stem.split("_")
                    
                    ckpt = CheckpointInfo(
                        path=str(ckpt_file),
                        epoch=int(name_parts[-2]) if len(name_parts) >= 2 else 0,
                        step=int(name_parts[-1]) if len(name_parts) >= 1 else 0,
                        timestamp=time.ctime(stat.st_mtime),
                        size_mb=stat.st_size / (1024 * 1024),
                        model_name=name_parts[0] if name_parts else "unknown"
                    )
                    
                    if model_name is None or ckpt.model_name == model_name:
                        checkpoints.append(ckpt)
                except Exception:
                    continue
        except Exception as e:
            self.logger.error(f"Error listing checkpoints: {e}")
        
        return sorted(checkpoints, key=lambda x: x.step, reverse=True)

    def schedule_checkpoint(self, model_name: str, interval_minutes: int,
                            callback: Optional[Callable] = None) -> Dict[str, Any]:
        if not self.is_feature_available():
            return {"success": False, "error": "Checkpoint scheduling requires AI Developer edition"}
        
        def checkpoint_loop():
            while self.active_training:
                time.sleep(interval_minutes * 60)
                if self.active_training and callback:
                    callback()
        
        self.active_training = True
        self.checkpoint_thread = threading.Thread(target=checkpoint_loop, daemon=True)
        self.checkpoint_thread.start()
        
        self.logger.info(f"Checkpoint scheduler started for {model_name} every {interval_minutes} minutes")
        return {"success": True, "model": model_name, "interval": interval_minutes}

    def stop_checkpoint_scheduler(self):
        self.active_training = False
        if self.checkpoint_thread:
            self.checkpoint_thread.join(timeout=5)
            self.checkpoint_thread = None

    def cleanup_old_checkpoints(self, model_name: str, keep_last: int = 5) -> Dict[str, Any]:
        checkpoints = self.list_checkpoints(model_name)
        
        if len(checkpoints) <= keep_last:
            return {"success": True, "deleted": 0}
        
        to_delete = checkpoints[keep_last:]
        deleted = 0
        
        for ckpt in to_delete:
            try:
                Path(ckpt.path).unlink()
                deleted += 1
            except Exception as e:
                self.logger.error(f"Failed to delete checkpoint {ckpt.path}: {e}")
        
        return {"success": True, "deleted": deleted}

    def setup_failure_recovery(self, model_name: str) -> Dict[str, Any]:
        if not self.is_feature_available():
            return {"success": False, "error": "Failure recovery requires AI Developer edition"}
        
        checkpoints = self.list_checkpoints(model_name)
        
        recovery_config = {
            "auto_resume": True,
            "latest_checkpoint": checkpoints[0].path if checkpoints else None,
            "checkpoint_validation": True,
            "graceful_shutdown": True,
            "oom_handling": True,
            "nan_detection": True
        }
        
        return {"success": True, "config": recovery_config}

    def get_status(self) -> Dict[str, Any]:
        gpu_mem = self.get_gpu_memory()
        
        return {
            "version": self.version,
            "license_tier": "aidev" if self.is_feature_available() else "limited",
            "gpu_memory": gpu_mem,
            "active_training": self.active_training,
            "checkpoints_count": len(self.list_checkpoints()),
            "config": self.config
        }

    def run_gui(self):
        if not GTK_AVAILABLE:
            return self.run_cli()
        
        win = TrainingOptimizerWindow(self)
        win.connect("destroy", Gtk.main_quit)
        win.show_all()
        Gtk.main()
    
    def run_cli(self):
        print(f"\n{'='*70}")
        print(f"  {APP_NAME} v{VERSION}")
        print(f"  License: {'AI Developer Edition' if self.is_feature_available() else 'LIMITED'}")
        print(f"{'='*70}\n")
        
        status = self.get_status()
        
        print("GPU Memory:")
        gpu = status["gpu_memory"]
        print(f"  Total: {gpu['total_mb']} MB")
        print(f"  Used: {gpu['used_mb']} MB")
        print(f"  Free: {gpu['free_mb']} MB")
        
        print(f"\nCheckpoints: {status['checkpoints_count']} saved")
        
        print("\nSupported Precision Modes:")
        for mode in ["fp32", "fp16", "bf16", "int8", "mixed"]:
            config = self.get_precision_config(mode)
            print(f"  {mode.upper()}: autocast={config['autocast']}, memory_efficient={config['memory_efficient']}")
        
        print("\nConfiguration:")
        for key, value in self.config.items():
            print(f"  {key}: {value}")


if GTK_AVAILABLE:
    class TrainingOptimizerWindow(Gtk.Window):
        def __init__(self, app: AegisTrainingOptimizer):
            super().__init__(title=f"{APP_NAME} v{VERSION}")
            self.app = app
            self.set_default_size(900, 700)
            self.set_border_width(10)
            self.setup_ui()
        
        def setup_ui(self):
            vbox = Gtk.Box(orientation=Gtk.Orientation.VERTICAL, spacing=10)
            self.add(vbox)
            
            header = Gtk.Label()
            header.set_markup(f"<big><b>{APP_NAME}</b></big>")
            vbox.pack_start(header, False, False, 10)
            
            notebook = Gtk.Notebook()
            vbox.pack_start(notebook, True, True, 0)
            
            notebook.append_page(self.create_batch_tab(), Gtk.Label(label="Batch Size"))
            notebook.append_page(self.create_vram_tab(), Gtk.Label(label="VRAM Planner"))
            notebook.append_page(self.create_precision_tab(), Gtk.Label(label="Precision"))
            notebook.append_page(self.create_checkpoint_tab(), Gtk.Label(label="Checkpoints"))
        
        def create_batch_tab(self):
            box = Gtk.Box(orientation=Gtk.Orientation.VERTICAL, spacing=10)
            box.set_margin_top(20)
            box.set_margin_start(20)
            
            params_box = Gtk.Box(orientation=Gtk.Orientation.HORIZONTAL, spacing=10)
            params_label = Gtk.Label(label="Model Parameters (millions):")
            self.params_entry = Gtk.Entry()
            self.params_entry.set_text("125")
            params_box.pack_start(params_label, False, False, 0)
            params_box.pack_start(self.params_entry, False, False, 0)
            box.pack_start(params_box, False, False, 10)
            
            calc_btn = Gtk.Button(label="Calculate Optimal Batch Size")
            calc_btn.connect("clicked", self.on_calculate_batch)
            calc_btn.set_sensitive(self.app.is_feature_available())
            box.pack_start(calc_btn, False, False, 10)
            
            self.batch_result = Gtk.Label()
            self.batch_result.set_xalign(0)
            box.pack_start(self.batch_result, False, False, 10)
            
            return box
        
        def create_vram_tab(self):
            box = Gtk.Box(orientation=Gtk.Orientation.VERTICAL, spacing=10)
            box.set_margin_top(20)
            box.set_margin_start(20)
            
            gpu_mem = self.app.get_gpu_memory()
            
            labels = [
                f"Total VRAM: {gpu_mem['total_mb']} MB",
                f"Used VRAM: {gpu_mem['used_mb']} MB",
                f"Free VRAM: {gpu_mem['free_mb']} MB"
            ]
            
            for text in labels:
                lbl = Gtk.Label(label=text)
                lbl.set_xalign(0)
                box.pack_start(lbl, False, False, 5)
            
            if gpu_mem['total_mb'] > 0:
                progress = Gtk.ProgressBar()
                progress.set_fraction(gpu_mem['used_mb'] / gpu_mem['total_mb'])
                progress.set_text(f"{int(gpu_mem['used_mb'] / gpu_mem['total_mb'] * 100)}% Used")
                progress.set_show_text(True)
                box.pack_start(progress, False, False, 20)
            
            return box
        
        def create_precision_tab(self):
            box = Gtk.Box(orientation=Gtk.Orientation.VERTICAL, spacing=10)
            box.set_margin_top(20)
            box.set_margin_start(20)
            
            for mode in ["fp32", "fp16", "bf16", "int8", "mixed"]:
                config = self.app.get_precision_config(mode)
                
                frame = Gtk.Frame(label=mode.upper())
                frame_box = Gtk.Box(orientation=Gtk.Orientation.VERTICAL, spacing=5)
                frame_box.set_margin_top(5)
                frame_box.set_margin_start(10)
                frame_box.set_margin_bottom(5)
                
                for key, value in config.items():
                    lbl = Gtk.Label(label=f"{key}: {value}")
                    lbl.set_xalign(0)
                    frame_box.pack_start(lbl, False, False, 2)
                
                frame.add(frame_box)
                box.pack_start(frame, False, False, 5)
            
            return box
        
        def create_checkpoint_tab(self):
            box = Gtk.Box(orientation=Gtk.Orientation.VERTICAL, spacing=10)
            box.set_margin_top(20)
            box.set_margin_start(20)
            
            checkpoints = self.app.list_checkpoints()
            
            if checkpoints:
                for ckpt in checkpoints[:10]:
                    hbox = Gtk.Box(orientation=Gtk.Orientation.HORIZONTAL, spacing=10)
                    info = f"{ckpt.model_name} - Epoch {ckpt.epoch}, Step {ckpt.step} ({ckpt.size_mb:.1f} MB)"
                    lbl = Gtk.Label(label=info)
                    lbl.set_xalign(0)
                    hbox.pack_start(lbl, True, True, 10)
                    box.pack_start(hbox, False, False, 5)
            else:
                no_ckpt = Gtk.Label(label="No checkpoints found")
                box.pack_start(no_ckpt, False, False, 20)
            
            return box
        
        def on_calculate_batch(self, button):
            try:
                params = float(self.params_entry.get_text())
                rec = self.app.recommend_batch_size(params)
                
                result_text = f"""
Recommended Batch Size: {rec.recommended_batch_size}
Maximum Batch Size: {rec.max_batch_size}
Memory per Sample: {rec.memory_per_sample_mb:.2f} MB
Gradient Accumulation: {rec.gradient_accumulation_steps}
Effective Batch Size: {rec.effective_batch_size}
"""
                self.batch_result.set_text(result_text)
            except ValueError:
                self.batch_result.set_text("Please enter a valid number")


def main():
    if not GTK_AVAILABLE:
        print("Cannot start Training Optimizer: GTK3 not available.", file=sys.stderr)
        sys.exit(1)
    parser = argparse.ArgumentParser(description=f"{APP_NAME}")
    parser.add_argument("--gui", action="store_true", help="Launch GUI mode")
    parser.add_argument("--cli", action="store_true", help="Run in CLI mode")
    parser.add_argument("--status", action="store_true", help="Show status")
    parser.add_argument("--recommend-batch", type=float, metavar="PARAMS_M",
                       help="Recommend batch size for model with N million parameters")
    parser.add_argument("--vram-plan", type=float, metavar="PARAMS_M",
                       help="Plan VRAM allocation for model")
    parser.add_argument("--precision", choices=["fp32", "fp16", "bf16", "int8", "mixed"],
                       help="Get precision configuration")
    parser.add_argument("--optimal-config", type=float, metavar="PARAMS_M",
                       help="Get optimal training configuration")
    parser.add_argument("--list-checkpoints", action="store_true", help="List saved checkpoints")
    parser.add_argument("--json", action="store_true", help="Output as JSON")
    parser.add_argument("--version", action="version", version=f"{APP_NAME} {VERSION}")
    
    args = parser.parse_args()
    
    if args.status:
        logging.disable(logging.CRITICAL)
        for handler in logging.root.handlers[:]:
            logging.root.removeHandler(handler)
        logging.root.addHandler(logging.NullHandler())
        try:
            app = AegisTrainingOptimizer(headless=True)
            batch_rec = app.recommend_batch_size(125)
            checkpoints = app.list_checkpoints()
            latest_checkpoint = checkpoints[0].path if checkpoints else None
            status = {
                "available": True,
                "active_job": None if not app.active_training else "training",
                "recommended_batch_size": batch_rec.recommended_batch_size,
                "checkpoint_path": latest_checkpoint,
                "version": VERSION
            }
            print(json.dumps(status))
            sys.exit(0)
        except Exception as e:
            print(json.dumps({"available": False, "error": str(e)}))
            sys.exit(1)
    elif args.recommend_batch:
        app = AegisTrainingOptimizer(headless=True)
        rec = app.recommend_batch_size(args.recommend_batch)
        print(json.dumps(asdict(rec), indent=2))
    elif args.vram_plan:
        app = AegisTrainingOptimizer(headless=True)
        alloc = app.calculate_vram_allocation(args.vram_plan)
        print(json.dumps(asdict(alloc), indent=2))
    elif args.precision:
        app = AegisTrainingOptimizer(headless=True)
        config = app.get_precision_config(args.precision)
        print(json.dumps(config, indent=2))
    elif args.optimal_config:
        app = AegisTrainingOptimizer(headless=True)
        config = app.get_optimal_training_config(args.optimal_config)
        print(json.dumps(asdict(config), indent=2))
    elif args.list_checkpoints:
        app = AegisTrainingOptimizer(headless=True)
        checkpoints = app.list_checkpoints()
        if args.json:
            print(json.dumps([asdict(c) for c in checkpoints], indent=2))
        else:
            for ckpt in checkpoints:
                print(f"{ckpt.model_name} - Epoch {ckpt.epoch}, Step {ckpt.step} - {ckpt.size_mb:.1f} MB")
    elif args.cli or not GTK_AVAILABLE:
        app = AegisTrainingOptimizer(headless=False)
        app.run_cli()
    else:
        app = AegisTrainingOptimizer(headless=False)
        app.run_gui()


if __name__ == "__main__":
    main()
