#!/usr/bin/env python3
"""
Aegis Dataset Manager - Dataset pipeline management
Features: Preprocessing, caching, chunking, memory mapping, large dataset support

Provides GUI (GTK) and CLI modes with tier-based feature gating.
"""

import os
import sys
import json
import subprocess
import logging
import argparse
import shutil
import hashlib
import mmap
import time
from pathlib import Path
from typing import Dict, List, Optional, Any, Iterator, Generator
from dataclasses import dataclass, asdict
from enum import Enum

TIER_LIMIT = "aidev"
VERSION = "1.5.0"
APP_NAME = "Aegis Dataset Manager"

CONFIG_FILE = "/etc/aegis/aidev-config.json"
LOG_FILE = "/var/log/aegis/dataset-manager.log"
CACHE_DIR = "/var/cache/aegis/datasets"
DATASETS_DIR = os.path.expanduser("~/Datasets")

try:
    import gi
    gi.require_version('Gtk', '3.0')
    from gi.repository import Gtk, GLib
    GTK_AVAILABLE = True
except ImportError:
    GTK_AVAILABLE = False
    print("Error: GTK3 is required. Install with: sudo pacman -S gtk3 python-gobject", file=sys.stderr)


class LicenseTier:
    FREEMIUM = 1
    BASIC = 2
    AIDEV = 4
    SERVER = 5


class DataFormat(Enum):
    CSV = "csv"
    JSON = "json"
    JSONL = "jsonl"
    PARQUET = "parquet"
    ARROW = "arrow"
    HDF5 = "hdf5"
    TFRECORD = "tfrecord"
    WEBDATASET = "webdataset"


@dataclass
class DatasetInfo:
    name: str
    path: str
    format: str
    size_mb: float
    num_samples: int
    cached: bool
    cache_path: Optional[str]
    columns: List[str]
    memory_mapped: bool


@dataclass
class PreprocessingPipeline:
    name: str
    steps: List[Dict[str, Any]]
    input_format: str
    output_format: str
    cache_intermediate: bool


@dataclass
class ChunkConfig:
    chunk_size_mb: int
    num_chunks: int
    overlap_samples: int
    shuffle_chunks: bool


@dataclass
class CacheStats:
    total_size_mb: float
    num_datasets: int
    hit_rate: float
    last_cleanup: str


class AegisDatasetManager:
    def __init__(self, headless: bool = False):
        self.headless = headless
        self.version = VERSION
        self.config = {}
        self.license_tier = LicenseTier.FREEMIUM
        self.cache_hits = 0
        self.cache_misses = 0
        
        self.setup_logging()
        self.load_license_tier()
        self.load_config()
        self.ensure_directories()
        
    def setup_logging(self):
        log_dir = Path(LOG_FILE).parent
        try:
            log_dir.mkdir(parents=True, exist_ok=True)
        except (PermissionError, OSError):
            pass
        
        try:
            logging.basicConfig(
                level=logging.INFO,
                format='%(asctime)s - %(levelname)s - [%(name)s] %(message)s',
                handlers=[
                    logging.FileHandler(LOG_FILE) if os.access(str(log_dir), os.W_OK) else logging.NullHandler(),
                    logging.StreamHandler() if not self.headless else logging.NullHandler()
                ]
            )
        except Exception:
            logging.basicConfig(level=logging.INFO, handlers=[logging.StreamHandler()])
        
        self.logger = logging.getLogger("AegisDatasetManager")
        self.logger.info(f"Starting {APP_NAME} v{VERSION}")
    
    def load_license_tier(self):
        license_file = Path("/etc/aegis/license.json")
        try:
            if license_file.exists():
                with open(license_file, 'r') as f:
                    license_data = json.load(f)
                edition = license_data.get('edition', 'freemium').lower()
                tier_map = {
                    'freemium': LicenseTier.FREEMIUM,
                    'basic': LicenseTier.BASIC,
                    'aidev': LicenseTier.AIDEV,
                    'server': LicenseTier.SERVER
                }
                self.license_tier = tier_map.get(edition, LicenseTier.FREEMIUM)
            else:
                if Path("/etc/aegis-aidev-marker").exists():
                    self.license_tier = LicenseTier.AIDEV
        except Exception as e:
            self.logger.warning(f"Failed to load license tier: {e}")
    
    def is_feature_available(self) -> bool:
        return self.license_tier >= LicenseTier.AIDEV
    
    def load_config(self):
        default_config = {
            "cache_enabled": True,
            "max_cache_size_gb": 50,
            "chunk_size_mb": 256,
            "prefetch_chunks": 2,
            "memory_map_threshold_mb": 100,
            "compression": "lz4",
            "num_workers": 4
        }
        
        try:
            if Path(CONFIG_FILE).exists():
                with open(CONFIG_FILE, 'r') as f:
                    file_config = json.load(f)
                    if "features" in file_config and "dataset_manager" in file_config["features"]:
                        self.config = {**default_config, **file_config["features"]["dataset_manager"]}
                    else:
                        self.config = default_config
            else:
                self.config = default_config
        except Exception as e:
            self.logger.error(f"Error loading config: {e}")
            self.config = default_config
    
    def ensure_directories(self):
        try:
            Path(CACHE_DIR).mkdir(parents=True, exist_ok=True)
        except (PermissionError, OSError):
            pass
        try:
            Path(DATASETS_DIR).mkdir(parents=True, exist_ok=True)
        except (PermissionError, OSError):
            pass

    def get_dataset_hash(self, path: str) -> str:
        hasher = hashlib.md5()
        hasher.update(path.encode())
        try:
            stat = Path(path).stat()
            hasher.update(str(stat.st_mtime).encode())
            hasher.update(str(stat.st_size).encode())
        except Exception:
            pass
        return hasher.hexdigest()[:16]

    def detect_format(self, path: str) -> str:
        path_lower = path.lower()
        format_map = {
            ".csv": "csv",
            ".json": "json",
            ".jsonl": "jsonl",
            ".parquet": "parquet",
            ".arrow": "arrow",
            ".h5": "hdf5",
            ".hdf5": "hdf5",
            ".tfrecord": "tfrecord",
            ".tar": "webdataset"
        }
        
        for ext, fmt in format_map.items():
            if path_lower.endswith(ext):
                return fmt
        
        return "unknown"

    def get_dataset_info(self, path: str) -> Optional[DatasetInfo]:
        if not Path(path).exists():
            return None
        
        try:
            stat = Path(path).stat()
            size_mb = stat.st_size / (1024 * 1024)
            
            fmt = self.detect_format(path)
            
            dataset_hash = self.get_dataset_hash(path)
            cache_path = Path(CACHE_DIR) / f"{dataset_hash}.cache"
            cached = cache_path.exists()
            
            num_samples = 0
            columns = []
            
            if fmt == "csv":
                try:
                    with open(path, 'r') as f:
                        header = f.readline().strip()
                        columns = header.split(',')
                        num_samples = sum(1 for _ in f)
                except Exception:
                    pass
            elif fmt == "jsonl":
                try:
                    with open(path, 'r') as f:
                        num_samples = sum(1 for _ in f)
                        f.seek(0)
                        first_line = json.loads(f.readline())
                        columns = list(first_line.keys()) if isinstance(first_line, dict) else []
                except Exception:
                    pass
            
            memory_mapped = size_mb >= self.config.get("memory_map_threshold_mb", 100)
            
            return DatasetInfo(
                name=Path(path).name,
                path=path,
                format=fmt,
                size_mb=size_mb,
                num_samples=num_samples,
                cached=cached,
                cache_path=str(cache_path) if cached else None,
                columns=columns,
                memory_mapped=memory_mapped
            )
        except Exception as e:
            self.logger.error(f"Error getting dataset info: {e}")
            return None

    def list_datasets(self) -> List[DatasetInfo]:
        datasets = []
        
        try:
            datasets_path = Path(DATASETS_DIR)
            if datasets_path.exists():
                for item in datasets_path.iterdir():
                    if item.is_file():
                        info = self.get_dataset_info(str(item))
                        if info:
                            datasets.append(info)
        except Exception as e:
            self.logger.error(f"Error listing datasets: {e}")
        
        return datasets

    def create_preprocessing_pipeline(self, name: str, steps: List[Dict[str, Any]],
                                        input_format: str = "csv",
                                        output_format: str = "arrow") -> PreprocessingPipeline:
        if not self.is_feature_available():
            return PreprocessingPipeline(
                name=name,
                steps=[],
                input_format=input_format,
                output_format=output_format,
                cache_intermediate=False
            )
        
        return PreprocessingPipeline(
            name=name,
            steps=steps,
            input_format=input_format,
            output_format=output_format,
            cache_intermediate=True
        )

    def get_default_preprocessing_steps(self) -> List[Dict[str, Any]]:
        return [
            {"type": "normalize", "method": "minmax"},
            {"type": "tokenize", "tokenizer": "bpe", "vocab_size": 32000},
            {"type": "pad", "max_length": 512, "padding": "max_length"},
            {"type": "encode", "dtype": "int32"}
        ]

    def cache_dataset(self, path: str, force: bool = False) -> Dict[str, Any]:
        if not self.is_feature_available():
            return {"success": False, "error": "Caching requires AI Developer edition"}
        
        info = self.get_dataset_info(path)
        if not info:
            return {"success": False, "error": "Dataset not found"}
        
        if info.cached and not force:
            self.cache_hits += 1
            return {"success": True, "cached": True, "cache_path": info.cache_path}
        
        self.cache_misses += 1
        
        try:
            dataset_hash = self.get_dataset_hash(path)
            cache_path = Path(CACHE_DIR) / f"{dataset_hash}.cache"
            
            shutil.copy2(path, cache_path)
            
            self.logger.info(f"Cached dataset: {path} -> {cache_path}")
            return {"success": True, "cached": True, "cache_path": str(cache_path)}
        except Exception as e:
            return {"success": False, "error": str(e)}

    def get_cache_stats(self) -> CacheStats:
        total_size = 0
        num_datasets = 0
        
        try:
            cache_path = Path(CACHE_DIR)
            if cache_path.exists():
                for item in cache_path.iterdir():
                    if item.is_file() and item.suffix == ".cache":
                        total_size += item.stat().st_size
                        num_datasets += 1
        except Exception:
            pass
        
        total_accesses = self.cache_hits + self.cache_misses
        hit_rate = (self.cache_hits / total_accesses * 100) if total_accesses > 0 else 0
        
        return CacheStats(
            total_size_mb=total_size / (1024 * 1024),
            num_datasets=num_datasets,
            hit_rate=hit_rate,
            last_cleanup=time.ctime()
        )

    def clear_cache(self, older_than_days: int = 0) -> Dict[str, Any]:
        if not self.is_feature_available():
            return {"success": False, "error": "Cache management requires AI Developer edition"}
        
        deleted = 0
        freed_mb = 0
        
        try:
            cache_path = Path(CACHE_DIR)
            if cache_path.exists():
                cutoff_time = time.time() - (older_than_days * 86400) if older_than_days > 0 else 0
                
                for item in cache_path.iterdir():
                    if item.is_file() and item.suffix == ".cache":
                        if older_than_days == 0 or item.stat().st_mtime < cutoff_time:
                            freed_mb += item.stat().st_size / (1024 * 1024)
                            item.unlink()
                            deleted += 1
        except Exception as e:
            return {"success": False, "error": str(e)}
        
        return {"success": True, "deleted": deleted, "freed_mb": freed_mb}

    def calculate_chunk_config(self, dataset_path: str, 
                                target_chunk_mb: int = 256) -> ChunkConfig:
        info = self.get_dataset_info(dataset_path)
        if not info:
            return ChunkConfig(chunk_size_mb=target_chunk_mb, num_chunks=1, 
                              overlap_samples=0, shuffle_chunks=True)
        
        num_chunks = max(1, int(info.size_mb / target_chunk_mb))
        samples_per_chunk = info.num_samples // num_chunks if num_chunks > 0 else info.num_samples
        overlap = int(samples_per_chunk * 0.01)
        
        return ChunkConfig(
            chunk_size_mb=target_chunk_mb,
            num_chunks=num_chunks,
            overlap_samples=overlap,
            shuffle_chunks=True
        )

    def create_memory_mapped_view(self, path: str) -> Dict[str, Any]:
        if not self.is_feature_available():
            return {"success": False, "error": "Memory mapping requires AI Developer edition"}
        
        if not Path(path).exists():
            return {"success": False, "error": "File not found"}
        
        try:
            file_size = Path(path).stat().st_size
            
            return {
                "success": True,
                "path": path,
                "size_bytes": file_size,
                "mapped": True,
                "access_mode": "read_only",
                "zero_copy": True
            }
        except Exception as e:
            return {"success": False, "error": str(e)}

    def stream_large_dataset(self, path: str, batch_size: int = 1000) -> Generator[Dict[str, Any], None, None]:
        if not Path(path).exists():
            yield {"error": "File not found"}
            return
        
        fmt = self.detect_format(path)
        
        if fmt == "jsonl":
            try:
                with open(path, 'r') as f:
                    batch = []
                    for line in f:
                        batch.append(json.loads(line.strip()))
                        if len(batch) >= batch_size:
                            yield {"batch": batch, "size": len(batch)}
                            batch = []
                    if batch:
                        yield {"batch": batch, "size": len(batch)}
            except Exception as e:
                yield {"error": str(e)}
        
        elif fmt == "csv":
            try:
                with open(path, 'r') as f:
                    header = f.readline().strip().split(',')
                    batch = []
                    for line in f:
                        values = line.strip().split(',')
                        row = dict(zip(header, values))
                        batch.append(row)
                        if len(batch) >= batch_size:
                            yield {"batch": batch, "size": len(batch)}
                            batch = []
                    if batch:
                        yield {"batch": batch, "size": len(batch)}
            except Exception as e:
                yield {"error": str(e)}
        else:
            yield {"error": f"Streaming not supported for format: {fmt}"}

    def get_ram_info(self) -> Dict[str, int]:
        ram_info = {"total_mb": 0, "available_mb": 0, "used_mb": 0}
        
        try:
            if Path("/proc/meminfo").exists():
                with open("/proc/meminfo", 'r') as f:
                    for line in f:
                        if "MemTotal" in line:
                            ram_info["total_mb"] = int(line.split()[1]) // 1024
                        elif "MemAvailable" in line:
                            ram_info["available_mb"] = int(line.split()[1]) // 1024
                
                ram_info["used_mb"] = ram_info["total_mb"] - ram_info["available_mb"]
        except Exception:
            pass
        
        return ram_info

    def get_status(self) -> Dict[str, Any]:
        cache_stats = self.get_cache_stats()
        ram_info = self.get_ram_info()
        
        return {
            "version": self.version,
            "license_tier": "aidev" if self.is_feature_available() else "limited",
            "datasets_dir": DATASETS_DIR,
            "cache_dir": CACHE_DIR,
            "cache_stats": asdict(cache_stats),
            "ram": ram_info,
            "datasets_count": len(self.list_datasets()),
            "config": self.config
        }

    def run_gui(self):
        if not GTK_AVAILABLE:
            return self.run_cli()
        
        win = DatasetManagerWindow(self)
        win.connect("destroy", Gtk.main_quit)
        win.show_all()
        Gtk.main()
    
    def run_cli(self):
        print(f"\n{'='*70}")
        print(f"  {APP_NAME} v{VERSION}")
        print(f"  License: {'AI Developer Edition' if self.is_feature_available() else 'LIMITED'}")
        print(f"{'='*70}\n")
        
        status = self.get_status()
        
        print("System Memory:")
        ram = status["ram"]
        print(f"  Total: {ram['total_mb']} MB")
        print(f"  Available: {ram['available_mb']} MB")
        print(f"  Used: {ram['used_mb']} MB")
        
        print(f"\nCache Statistics:")
        cache = status["cache_stats"]
        print(f"  Size: {cache['total_size_mb']:.1f} MB")
        print(f"  Datasets: {cache['num_datasets']}")
        print(f"  Hit Rate: {cache['hit_rate']:.1f}%")
        
        print(f"\nDatasets ({status['datasets_count']} found):")
        for dataset in self.list_datasets()[:10]:
            cached = "✓" if dataset.cached else "✗"
            mmap = "[mmap]" if dataset.memory_mapped else ""
            print(f"  {cached} {dataset.name} - {dataset.format} - {dataset.size_mb:.1f} MB {mmap}")
        
        print("\nConfiguration:")
        for key, value in self.config.items():
            print(f"  {key}: {value}")


if GTK_AVAILABLE:
    class DatasetManagerWindow(Gtk.Window):
        def __init__(self, app: AegisDatasetManager):
            super().__init__(title=f"{APP_NAME} v{VERSION}")
            self.app = app
            self.set_default_size(900, 700)
            self.set_border_width(10)
            self.setup_ui()
        
        def setup_ui(self):
            vbox = Gtk.Box(orientation=Gtk.Orientation.VERTICAL, spacing=10)
            self.add(vbox)
            
            header = Gtk.Label()
            header.set_markup(f"<big><b>{APP_NAME}</b></big>")
            vbox.pack_start(header, False, False, 10)
            
            notebook = Gtk.Notebook()
            vbox.pack_start(notebook, True, True, 0)
            
            notebook.append_page(self.create_datasets_tab(), Gtk.Label(label="Datasets"))
            notebook.append_page(self.create_cache_tab(), Gtk.Label(label="Cache"))
            notebook.append_page(self.create_pipeline_tab(), Gtk.Label(label="Preprocessing"))
            notebook.append_page(self.create_memory_tab(), Gtk.Label(label="Memory"))
        
        def create_datasets_tab(self):
            box = Gtk.Box(orientation=Gtk.Orientation.VERTICAL, spacing=10)
            box.set_margin_top(20)
            box.set_margin_start(20)
            
            datasets = self.app.list_datasets()
            
            for dataset in datasets[:15]:
                hbox = Gtk.Box(orientation=Gtk.Orientation.HORIZONTAL, spacing=10)
                
                status = "✓" if dataset.cached else "○"
                info = f"{status} {dataset.name} ({dataset.format}) - {dataset.size_mb:.1f} MB"
                if dataset.num_samples > 0:
                    info += f" - {dataset.num_samples} samples"
                
                label = Gtk.Label(label=info)
                label.set_xalign(0)
                hbox.pack_start(label, True, True, 10)
                
                cache_btn = Gtk.Button(label="Cache")
                cache_btn.connect("clicked", self.on_cache_dataset, dataset.path)
                cache_btn.set_sensitive(self.app.is_feature_available() and not dataset.cached)
                hbox.pack_end(cache_btn, False, False, 5)
                
                box.pack_start(hbox, False, False, 5)
            
            if not datasets:
                no_data = Gtk.Label(label=f"No datasets found in {DATASETS_DIR}")
                box.pack_start(no_data, False, False, 20)
            
            return box
        
        def create_cache_tab(self):
            box = Gtk.Box(orientation=Gtk.Orientation.VERTICAL, spacing=10)
            box.set_margin_top(20)
            box.set_margin_start(20)
            
            stats = self.app.get_cache_stats()
            
            labels = [
                f"Cache Size: {stats.total_size_mb:.1f} MB",
                f"Cached Datasets: {stats.num_datasets}",
                f"Hit Rate: {stats.hit_rate:.1f}%"
            ]
            
            for text in labels:
                lbl = Gtk.Label(label=text)
                lbl.set_xalign(0)
                box.pack_start(lbl, False, False, 5)
            
            clear_btn = Gtk.Button(label="Clear Cache")
            clear_btn.connect("clicked", self.on_clear_cache)
            clear_btn.set_sensitive(self.app.is_feature_available())
            box.pack_start(clear_btn, False, False, 20)
            
            return box
        
        def create_pipeline_tab(self):
            box = Gtk.Box(orientation=Gtk.Orientation.VERTICAL, spacing=10)
            box.set_margin_top(20)
            box.set_margin_start(20)
            
            title = Gtk.Label()
            title.set_markup("<b>Default Preprocessing Steps:</b>")
            title.set_xalign(0)
            box.pack_start(title, False, False, 10)
            
            steps = self.app.get_default_preprocessing_steps()
            for i, step in enumerate(steps, 1):
                step_text = f"{i}. {step['type'].capitalize()}"
                if 'method' in step:
                    step_text += f" (method: {step['method']})"
                elif 'max_length' in step:
                    step_text += f" (max_length: {step['max_length']})"
                
                lbl = Gtk.Label(label=step_text)
                lbl.set_xalign(0)
                box.pack_start(lbl, False, False, 5)
            
            return box
        
        def create_memory_tab(self):
            box = Gtk.Box(orientation=Gtk.Orientation.VERTICAL, spacing=10)
            box.set_margin_top(20)
            box.set_margin_start(20)
            
            ram = self.app.get_ram_info()
            
            labels = [
                f"Total RAM: {ram['total_mb']} MB",
                f"Available: {ram['available_mb']} MB",
                f"Used: {ram['used_mb']} MB"
            ]
            
            for text in labels:
                lbl = Gtk.Label(label=text)
                lbl.set_xalign(0)
                box.pack_start(lbl, False, False, 5)
            
            if ram['total_mb'] > 0:
                progress = Gtk.ProgressBar()
                progress.set_fraction(ram['used_mb'] / ram['total_mb'])
                progress.set_text(f"{int(ram['used_mb'] / ram['total_mb'] * 100)}% Used")
                progress.set_show_text(True)
                box.pack_start(progress, False, False, 20)
            
            mmap_info = Gtk.Label()
            mmap_info.set_markup(f"<i>Memory mapping threshold: {self.app.config.get('memory_map_threshold_mb', 100)} MB</i>")
            mmap_info.set_xalign(0)
            box.pack_start(mmap_info, False, False, 10)
            
            return box
        
        def on_cache_dataset(self, button, path):
            result = self.app.cache_dataset(path)
            dialog = Gtk.MessageDialog(
                transient_for=self,
                flags=0,
                message_type=Gtk.MessageType.INFO if result["success"] else Gtk.MessageType.ERROR,
                buttons=Gtk.ButtonsType.OK,
                text="Cached successfully" if result["success"] else result.get("error", "Failed")
            )
            dialog.run()
            dialog.destroy()
        
        def on_clear_cache(self, button):
            result = self.app.clear_cache()
            dialog = Gtk.MessageDialog(
                transient_for=self,
                flags=0,
                message_type=Gtk.MessageType.INFO,
                buttons=Gtk.ButtonsType.OK,
                text=f"Cleared {result.get('deleted', 0)} cached datasets, freed {result.get('freed_mb', 0):.1f} MB"
            )
            dialog.run()
            dialog.destroy()


def main():
    if not GTK_AVAILABLE:
        print("Cannot start Dataset Manager: GTK3 not available.", file=sys.stderr)
        sys.exit(1)
    parser = argparse.ArgumentParser(description=f"{APP_NAME}")
    parser.add_argument("--gui", action="store_true", help="Launch GUI mode")
    parser.add_argument("--cli", action="store_true", help="Run in CLI mode")
    parser.add_argument("--status", action="store_true", help="Show status")
    parser.add_argument("--list", action="store_true", help="List datasets")
    parser.add_argument("--info", metavar="PATH", help="Get dataset info")
    parser.add_argument("--cache", metavar="PATH", help="Cache a dataset")
    parser.add_argument("--cache-stats", action="store_true", help="Show cache statistics")
    parser.add_argument("--clear-cache", action="store_true", help="Clear cache")
    parser.add_argument("--chunk-config", metavar="PATH", help="Get chunking configuration")
    parser.add_argument("--json", action="store_true", help="Output as JSON")
    parser.add_argument("--version", action="version", version=f"{APP_NAME} {VERSION}")
    
    args = parser.parse_args()
    
    if args.status:
        logging.disable(logging.CRITICAL)
        for handler in logging.root.handlers[:]:
            logging.root.removeHandler(handler)
        logging.root.addHandler(logging.NullHandler())
        try:
            app = AegisDatasetManager(headless=True)
            datasets = app.list_datasets()
            cache_stats = app.get_cache_stats()
            dataset_list = []
            for d in datasets:
                dataset_list.append({
                    "name": d.name,
                    "path": d.path,
                    "format": d.format,
                    "size_mb": d.size_mb
                })
            status = {
                "available": True,
                "datasets": dataset_list,
                "cache_size_mb": cache_stats.total_size_mb,
                "cache_hit_rate": cache_stats.hit_rate,
                "version": VERSION
            }
            print(json.dumps(status))
            sys.exit(0)
        except Exception as e:
            print(json.dumps({"available": False, "error": str(e)}))
            sys.exit(1)
    elif args.list:
        app = AegisDatasetManager(headless=True)
        datasets = app.list_datasets()
        if args.json:
            print(json.dumps([asdict(d) for d in datasets], indent=2))
        else:
            for d in datasets:
                cached = "✓" if d.cached else "✗"
                print(f"{cached} {d.name} - {d.format} - {d.size_mb:.1f} MB - {d.num_samples} samples")
    elif args.info:
        app = AegisDatasetManager(headless=True)
        info = app.get_dataset_info(args.info)
        if info:
            print(json.dumps(asdict(info), indent=2))
        else:
            print("Dataset not found")
    elif args.cache:
        app = AegisDatasetManager(headless=True)
        result = app.cache_dataset(args.cache)
        print(json.dumps(result, indent=2))
    elif args.cache_stats:
        app = AegisDatasetManager(headless=True)
        stats = app.get_cache_stats()
        print(json.dumps(asdict(stats), indent=2))
    elif args.clear_cache:
        app = AegisDatasetManager(headless=True)
        result = app.clear_cache()
        print(json.dumps(result, indent=2))
    elif args.chunk_config:
        app = AegisDatasetManager(headless=True)
        config = app.calculate_chunk_config(args.chunk_config)
        print(json.dumps(asdict(config), indent=2))
    elif args.cli or not GTK_AVAILABLE:
        app = AegisDatasetManager(headless=False)
        app.run_cli()
    else:
        app = AegisDatasetManager(headless=False)
        app.run_gui()


if __name__ == "__main__":
    main()
