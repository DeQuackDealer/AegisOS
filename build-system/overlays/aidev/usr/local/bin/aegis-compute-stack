#!/usr/bin/env python3
"""
Aegis Compute Stack - Unified hardware acceleration API
Features: CUDA/ROCm/Vulkan/OpenCL detection, tensor-core config, unified API

Provides GUI (GTK) and CLI modes with tier-based feature gating.
"""

import os
import sys
import json
import subprocess
import logging
import argparse
import shutil
import re
from pathlib import Path
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, asdict
from enum import Enum

TIER_LIMIT = "aidev"
VERSION = "1.5.0"
APP_NAME = "Aegis Compute Stack"

CONFIG_FILE = "/etc/aegis/aidev-config.json"
LOG_FILE = "/var/log/aegis/compute-stack.log"

try:
    import gi
    gi.require_version('Gtk', '3.0')
    from gi.repository import Gtk, GLib, Pango
    GTK_AVAILABLE = True
except ImportError:
    GTK_AVAILABLE = False


class LicenseTier:
    FREEMIUM = 1
    BASIC = 2
    AIDEV = 4
    SERVER = 5


class ComputeBackend(Enum):
    CUDA = "cuda"
    ROCM = "rocm"
    VULKAN = "vulkan"
    OPENCL = "opencl"
    CPU = "cpu"


@dataclass
class GPUDevice:
    index: int
    name: str
    vendor: str
    memory_total_mb: int
    memory_free_mb: int
    compute_capability: str
    driver_version: str
    backend: ComputeBackend
    tensor_cores: bool
    multi_instance_gpu: bool


@dataclass
class ComputeCapabilities:
    backends_available: List[str]
    primary_backend: str
    gpus: List[Dict]
    tensor_cores_available: bool
    mixed_precision_support: bool
    multi_gpu_support: bool
    total_vram_mb: int
    cpu_cores: int
    cpu_threads: int


class AegisComputeStack:
    def __init__(self, headless: bool = False):
        self.headless = headless
        self.version = VERSION
        self.config = {}
        self.license_tier = LicenseTier.FREEMIUM
        self.gpus: List[GPUDevice] = []
        self.primary_backend = ComputeBackend.CPU
        
        self.setup_logging()
        self.load_license_tier()
        self.load_config()
        
    def setup_logging(self):
        log_dir = Path(LOG_FILE).parent
        try:
            log_dir.mkdir(parents=True, exist_ok=True)
        except (PermissionError, OSError):
            pass
        
        try:
            logging.basicConfig(
                level=logging.INFO,
                format='%(asctime)s - %(levelname)s - [%(name)s] %(message)s',
                handlers=[
                    logging.FileHandler(LOG_FILE) if os.access(str(log_dir), os.W_OK) else logging.NullHandler(),
                    logging.StreamHandler() if not self.headless else logging.NullHandler()
                ]
            )
        except Exception:
            logging.basicConfig(level=logging.INFO, handlers=[logging.StreamHandler()])
        
        self.logger = logging.getLogger("AegisComputeStack")
        self.logger.info(f"Starting {APP_NAME} v{VERSION}")
    
    def load_license_tier(self):
        license_file = Path("/etc/aegis/license.json")
        try:
            if license_file.exists():
                with open(license_file, 'r') as f:
                    license_data = json.load(f)
                edition = license_data.get('edition', 'freemium').lower()
                tier_map = {
                    'freemium': LicenseTier.FREEMIUM,
                    'basic': LicenseTier.BASIC,
                    'aidev': LicenseTier.AIDEV,
                    'server': LicenseTier.SERVER
                }
                self.license_tier = tier_map.get(edition, LicenseTier.FREEMIUM)
            else:
                if Path("/etc/aegis-aidev-marker").exists():
                    self.license_tier = LicenseTier.AIDEV
        except Exception as e:
            self.logger.warning(f"Failed to load license tier: {e}")
    
    def is_feature_available(self) -> bool:
        return self.license_tier >= LicenseTier.AIDEV
    
    def load_config(self):
        default_config = {
            "preferred_backend": "auto",
            "gpu_power_limit": 100,
            "enable_tensor_cores": True,
            "mixed_precision": "fp16",
            "multi_gpu_strategy": "data_parallel"
        }
        
        try:
            if Path(CONFIG_FILE).exists():
                with open(CONFIG_FILE, 'r') as f:
                    file_config = json.load(f)
                    if "features" in file_config and "compute_stack" in file_config["features"]:
                        self.config = {**default_config, **file_config["features"]["compute_stack"]}
                    else:
                        self.config = default_config
            else:
                self.config = default_config
        except Exception as e:
            self.logger.error(f"Error loading config: {e}")
            self.config = default_config

    def check_nvidia_smi(self) -> bool:
        return shutil.which("nvidia-smi") is not None
    
    def check_nvcc(self) -> bool:
        return shutil.which("nvcc") is not None
    
    def check_rocm_smi(self) -> bool:
        return shutil.which("rocm-smi") is not None or shutil.which("rocminfo") is not None
    
    def check_vulkan(self) -> bool:
        return shutil.which("vulkaninfo") is not None or Path("/usr/share/vulkan").exists()
    
    def check_opencl(self) -> bool:
        return shutil.which("clinfo") is not None or Path("/etc/OpenCL/vendors").exists()

    def detect_nvidia_gpus(self) -> List[GPUDevice]:
        gpus = []
        if not self.check_nvidia_smi():
            return gpus
        
        try:
            result = subprocess.run(
                ["nvidia-smi", "--query-gpu=index,name,memory.total,memory.free,driver_version,compute_cap", 
                 "--format=csv,noheader,nounits"],
                capture_output=True, text=True, timeout=10
            )
            
            if result.returncode == 0:
                for line in result.stdout.strip().split('\n'):
                    if not line.strip():
                        continue
                    parts = [p.strip() for p in line.split(',')]
                    if len(parts) >= 6:
                        compute_cap = parts[5]
                        major_ver = int(compute_cap.split('.')[0]) if '.' in compute_cap else int(compute_cap)
                        
                        gpu = GPUDevice(
                            index=int(parts[0]),
                            name=parts[1],
                            vendor="NVIDIA",
                            memory_total_mb=int(float(parts[2])),
                            memory_free_mb=int(float(parts[3])),
                            driver_version=parts[4],
                            compute_capability=compute_cap,
                            backend=ComputeBackend.CUDA,
                            tensor_cores=major_ver >= 7,
                            multi_instance_gpu=major_ver >= 8
                        )
                        gpus.append(gpu)
        except Exception as e:
            self.logger.error(f"Error detecting NVIDIA GPUs: {e}")
        
        return gpus
    
    def detect_amd_gpus(self) -> List[GPUDevice]:
        gpus = []
        if not self.check_rocm_smi():
            return gpus
        
        try:
            result = subprocess.run(
                ["rocm-smi", "--showid", "--showmeminfo", "vram", "--json"],
                capture_output=True, text=True, timeout=10
            )
            
            if result.returncode == 0:
                try:
                    data = json.loads(result.stdout)
                    for card_id, card_info in data.items():
                        if card_id.startswith("card"):
                            idx = int(card_id.replace("card", ""))
                            gpu = GPUDevice(
                                index=idx,
                                name=card_info.get("Card series", "AMD GPU"),
                                vendor="AMD",
                                memory_total_mb=int(card_info.get("VRAM Total Memory (B)", 0)) // (1024*1024),
                                memory_free_mb=int(card_info.get("VRAM Total Used Memory (B)", 0)) // (1024*1024),
                                driver_version=card_info.get("Driver version", "unknown"),
                                compute_capability="gfx" + card_info.get("GPU ID", "unknown"),
                                backend=ComputeBackend.ROCM,
                                tensor_cores=False,
                                multi_instance_gpu=False
                            )
                            gpus.append(gpu)
                except json.JSONDecodeError:
                    pass
        except subprocess.TimeoutExpired:
            pass
        except FileNotFoundError:
            pass
        except Exception as e:
            self.logger.error(f"Error detecting AMD GPUs: {e}")
        
        return gpus
    
    def detect_vulkan_devices(self) -> List[Dict[str, Any]]:
        devices = []
        if not self.check_vulkan():
            return devices
        
        try:
            result = subprocess.run(
                ["vulkaninfo", "--summary"],
                capture_output=True, text=True, timeout=10
            )
            
            if result.returncode == 0:
                lines = result.stdout.split('\n')
                current_device = {}
                for line in lines:
                    if "deviceName" in line:
                        if current_device:
                            devices.append(current_device)
                        current_device = {"name": line.split('=')[-1].strip()}
                    elif "deviceType" in line and current_device:
                        current_device["type"] = line.split('=')[-1].strip()
                    elif "apiVersion" in line and current_device:
                        current_device["api_version"] = line.split('=')[-1].strip()
                
                if current_device:
                    devices.append(current_device)
        except Exception as e:
            self.logger.error(f"Error detecting Vulkan devices: {e}")
        
        return devices
    
    def detect_opencl_devices(self) -> List[Dict[str, Any]]:
        devices = []
        if not self.check_opencl():
            return devices
        
        try:
            result = subprocess.run(
                ["clinfo", "-l"],
                capture_output=True, text=True, timeout=10
            )
            
            if result.returncode == 0:
                for line in result.stdout.split('\n'):
                    if 'Device' in line and 'Platform' not in line:
                        device_name = line.split(':')[-1].strip() if ':' in line else line.strip()
                        if device_name:
                            devices.append({"name": device_name, "type": "GPU/CPU"})
        except Exception as e:
            self.logger.error(f"Error detecting OpenCL devices: {e}")
        
        return devices

    def get_cpu_info(self) -> Dict[str, Any]:
        cpu_info = {"cores": 1, "threads": 1, "model": "Unknown"}
        
        try:
            if Path("/proc/cpuinfo").exists():
                with open("/proc/cpuinfo", 'r') as f:
                    content = f.read()
                    
                cores = content.count("processor")
                cpu_info["threads"] = cores if cores > 0 else 1
                
                for line in content.split('\n'):
                    if "model name" in line:
                        cpu_info["model"] = line.split(':')[-1].strip()
                        break
                    if "cpu cores" in line:
                        cpu_info["cores"] = int(line.split(':')[-1].strip())
        except Exception as e:
            self.logger.error(f"Error getting CPU info: {e}")
        
        return cpu_info

    def detect_all_backends(self) -> Dict[str, bool]:
        return {
            "cuda": self.check_nvidia_smi() or self.check_nvcc(),
            "rocm": self.check_rocm_smi(),
            "vulkan": self.check_vulkan(),
            "opencl": self.check_opencl(),
            "cpu": True
        }

    def scan_hardware(self) -> ComputeCapabilities:
        backends = self.detect_all_backends()
        available_backends = [k for k, v in backends.items() if v]
        
        self.gpus = []
        self.gpus.extend(self.detect_nvidia_gpus())
        self.gpus.extend(self.detect_amd_gpus())
        
        if backends["cuda"] and any(g.backend == ComputeBackend.CUDA for g in self.gpus):
            self.primary_backend = ComputeBackend.CUDA
        elif backends["rocm"] and any(g.backend == ComputeBackend.ROCM for g in self.gpus):
            self.primary_backend = ComputeBackend.ROCM
        elif backends["vulkan"]:
            self.primary_backend = ComputeBackend.VULKAN
        elif backends["opencl"]:
            self.primary_backend = ComputeBackend.OPENCL
        else:
            self.primary_backend = ComputeBackend.CPU
        
        cpu_info = self.get_cpu_info()
        
        total_vram = sum(g.memory_total_mb for g in self.gpus)
        tensor_cores = any(g.tensor_cores for g in self.gpus)
        
        capabilities = ComputeCapabilities(
            backends_available=available_backends,
            primary_backend=self.primary_backend.value,
            gpus=[asdict(g) for g in self.gpus],
            tensor_cores_available=tensor_cores,
            mixed_precision_support=tensor_cores or len(self.gpus) > 0,
            multi_gpu_support=len(self.gpus) > 1,
            total_vram_mb=total_vram,
            cpu_cores=cpu_info["cores"],
            cpu_threads=cpu_info["threads"]
        )
        
        return capabilities

    def get_unified_api_status(self) -> Dict[str, Any]:
        capabilities = self.scan_hardware()
        
        return {
            "version": self.version,
            "license_tier": "aidev" if self.is_feature_available() else "limited",
            "capabilities": asdict(capabilities) if hasattr(capabilities, '__dict__') else {
                "backends_available": capabilities.backends_available,
                "primary_backend": capabilities.primary_backend,
                "gpus": capabilities.gpus,
                "tensor_cores_available": capabilities.tensor_cores_available,
                "mixed_precision_support": capabilities.mixed_precision_support,
                "multi_gpu_support": capabilities.multi_gpu_support,
                "total_vram_mb": capabilities.total_vram_mb,
                "cpu_cores": capabilities.cpu_cores,
                "cpu_threads": capabilities.cpu_threads
            },
            "config": self.config
        }

    def configure_tensor_cores(self, enabled: bool = True, precision: str = "tf32") -> Dict[str, Any]:
        if not self.is_feature_available():
            return {"success": False, "error": "Tensor core configuration requires AI Developer edition"}
        
        if not any(g.tensor_cores for g in self.gpus):
            return {"success": False, "error": "No tensor cores detected"}
        
        valid_precisions = ["tf32", "fp16", "bf16", "int8"]
        if precision not in valid_precisions:
            return {"success": False, "error": f"Invalid precision. Use: {valid_precisions}"}
        
        self.config["enable_tensor_cores"] = enabled
        self.config["tensor_precision"] = precision
        
        self.logger.info(f"Tensor cores configured: enabled={enabled}, precision={precision}")
        return {"success": True, "enabled": enabled, "precision": precision}

    def set_gpu_power_limit(self, device_id: int, power_percent: int) -> Dict[str, Any]:
        if not self.is_feature_available():
            return {"success": False, "error": "Power management requires AI Developer edition"}
        
        if power_percent < 50 or power_percent > 100:
            return {"success": False, "error": "Power limit must be between 50-100%"}
        
        if self.check_nvidia_smi():
            try:
                result = subprocess.run(
                    ["nvidia-smi", "-i", str(device_id), "-pl", str(power_percent)],
                    capture_output=True, text=True, timeout=10
                )
                if result.returncode == 0:
                    return {"success": True, "device": device_id, "power_limit": power_percent}
            except Exception as e:
                return {"success": False, "error": str(e)}
        
        return {"success": False, "error": "Power limit setting not supported for this GPU"}

    def get_backend_for_framework(self, framework: str) -> str:
        framework_backends = {
            "pytorch": ["cuda", "rocm", "cpu"],
            "tensorflow": ["cuda", "rocm", "cpu"],
            "jax": ["cuda", "rocm", "cpu"],
            "onnx": ["cuda", "rocm", "cpu"],
            "triton": ["cuda"]
        }
        
        preferred = framework_backends.get(framework.lower(), ["cpu"])
        backends = self.detect_all_backends()
        
        for backend in preferred:
            if backends.get(backend, False):
                return backend
        
        return "cpu"

    def run_gui(self):
        if not GTK_AVAILABLE:
            return self.run_cli()
        
        win = ComputeStackWindow(self)
        win.connect("destroy", Gtk.main_quit)
        win.show_all()
        Gtk.main()
    
    def run_cli(self):
        print(f"\n{'='*70}")
        print(f"  {APP_NAME} v{VERSION}")
        print(f"  License: {'AI Developer Edition' if self.is_feature_available() else 'LIMITED'}")
        print(f"{'='*70}\n")
        
        status = self.get_unified_api_status()
        caps = status["capabilities"]
        
        print("Compute Backends:")
        for backend in caps["backends_available"]:
            primary = " (PRIMARY)" if backend == caps["primary_backend"] else ""
            print(f"  ✓ {backend.upper()}{primary}")
        
        print(f"\nGPU Devices ({len(caps['gpus'])} found):")
        if caps["gpus"]:
            for gpu in caps["gpus"]:
                tensor = " [Tensor Cores]" if gpu.get("tensor_cores") else ""
                print(f"  [{gpu['index']}] {gpu['name']} - {gpu['memory_total_mb']}MB VRAM{tensor}")
                print(f"      Driver: {gpu['driver_version']}, Compute: {gpu['compute_capability']}")
        else:
            print("  No GPU devices detected")
        
        print(f"\nCapabilities:")
        print(f"  Tensor Cores: {'✓' if caps['tensor_cores_available'] else '✗'}")
        print(f"  Mixed Precision: {'✓' if caps['mixed_precision_support'] else '✗'}")
        print(f"  Multi-GPU: {'✓' if caps['multi_gpu_support'] else '✗'}")
        print(f"  Total VRAM: {caps['total_vram_mb']} MB")
        print(f"  CPU: {caps['cpu_cores']} cores / {caps['cpu_threads']} threads")
        
        print(f"\nConfiguration:")
        for key, value in self.config.items():
            print(f"  {key}: {value}")


if GTK_AVAILABLE:
    class ComputeStackWindow(Gtk.Window):
        def __init__(self, app: AegisComputeStack):
            super().__init__(title=f"{APP_NAME} v{VERSION}")
            self.app = app
            self.set_default_size(900, 700)
            self.set_border_width(10)
            self.setup_ui()
        
        def setup_ui(self):
            vbox = Gtk.Box(orientation=Gtk.Orientation.VERTICAL, spacing=10)
            self.add(vbox)
            
            header = Gtk.Label()
            header.set_markup(f"<big><b>{APP_NAME}</b></big>")
            vbox.pack_start(header, False, False, 10)
            
            tier_label = Gtk.Label()
            tier_text = "AI Developer Edition" if self.app.is_feature_available() else "Limited Mode"
            tier_label.set_markup(f"<i>License: {tier_text}</i>")
            vbox.pack_start(tier_label, False, False, 5)
            
            notebook = Gtk.Notebook()
            vbox.pack_start(notebook, True, True, 0)
            
            notebook.append_page(self.create_overview_tab(), Gtk.Label(label="Overview"))
            notebook.append_page(self.create_gpus_tab(), Gtk.Label(label="GPU Devices"))
            notebook.append_page(self.create_backends_tab(), Gtk.Label(label="Backends"))
            notebook.append_page(self.create_config_tab(), Gtk.Label(label="Configuration"))
            
            refresh_btn = Gtk.Button(label="Refresh Hardware Scan")
            refresh_btn.connect("clicked", self.on_refresh)
            vbox.pack_start(refresh_btn, False, False, 10)
        
        def create_overview_tab(self):
            box = Gtk.Box(orientation=Gtk.Orientation.VERTICAL, spacing=10)
            box.set_margin_top(20)
            box.set_margin_start(20)
            
            status = self.app.get_unified_api_status()
            caps = status["capabilities"]
            
            grid = Gtk.Grid()
            grid.set_column_spacing(20)
            grid.set_row_spacing(10)
            
            labels = [
                ("Primary Backend:", caps["primary_backend"].upper()),
                ("Total VRAM:", f"{caps['total_vram_mb']} MB"),
                ("GPU Count:", str(len(caps["gpus"]))),
                ("Tensor Cores:", "Available" if caps["tensor_cores_available"] else "Not Available"),
                ("Mixed Precision:", "Supported" if caps["mixed_precision_support"] else "Not Supported"),
                ("Multi-GPU:", "Supported" if caps["multi_gpu_support"] else "Single GPU"),
                ("CPU Cores:", str(caps["cpu_cores"])),
                ("CPU Threads:", str(caps["cpu_threads"]))
            ]
            
            for i, (label, value) in enumerate(labels):
                lbl = Gtk.Label(label=label)
                lbl.set_xalign(0)
                val = Gtk.Label(label=value)
                val.set_xalign(0)
                grid.attach(lbl, 0, i, 1, 1)
                grid.attach(val, 1, i, 1, 1)
            
            box.pack_start(grid, False, False, 10)
            return box
        
        def create_gpus_tab(self):
            box = Gtk.Box(orientation=Gtk.Orientation.VERTICAL, spacing=10)
            box.set_margin_top(20)
            box.set_margin_start(20)
            
            status = self.app.get_unified_api_status()
            
            for gpu in status["capabilities"]["gpus"]:
                frame = Gtk.Frame(label=f"GPU {gpu['index']}: {gpu['name']}")
                frame_box = Gtk.Box(orientation=Gtk.Orientation.VERTICAL, spacing=5)
                frame_box.set_margin_top(10)
                frame_box.set_margin_start(10)
                frame_box.set_margin_bottom(10)
                
                info_labels = [
                    f"Vendor: {gpu['vendor']}",
                    f"VRAM: {gpu['memory_total_mb']} MB (Free: {gpu['memory_free_mb']} MB)",
                    f"Driver: {gpu['driver_version']}",
                    f"Compute Capability: {gpu['compute_capability']}",
                    f"Backend: {gpu['backend']}",
                    f"Tensor Cores: {'Yes' if gpu['tensor_cores'] else 'No'}"
                ]
                
                for info in info_labels:
                    lbl = Gtk.Label(label=info)
                    lbl.set_xalign(0)
                    frame_box.pack_start(lbl, False, False, 2)
                
                frame.add(frame_box)
                box.pack_start(frame, False, False, 10)
            
            if not status["capabilities"]["gpus"]:
                no_gpu = Gtk.Label(label="No GPU devices detected")
                box.pack_start(no_gpu, False, False, 20)
            
            return box
        
        def create_backends_tab(self):
            box = Gtk.Box(orientation=Gtk.Orientation.VERTICAL, spacing=10)
            box.set_margin_top(20)
            box.set_margin_start(20)
            
            backends = self.app.detect_all_backends()
            
            for backend, available in backends.items():
                hbox = Gtk.Box(orientation=Gtk.Orientation.HORIZONTAL, spacing=10)
                
                status_icon = "✓" if available else "✗"
                color = "green" if available else "red"
                label = Gtk.Label()
                label.set_markup(f"<span foreground='{color}'>{status_icon}</span> {backend.upper()}")
                label.set_xalign(0)
                hbox.pack_start(label, True, True, 10)
                
                box.pack_start(hbox, False, False, 5)
            
            return box
        
        def create_config_tab(self):
            box = Gtk.Box(orientation=Gtk.Orientation.VERTICAL, spacing=10)
            box.set_margin_top(20)
            box.set_margin_start(20)
            
            tensor_check = Gtk.CheckButton(label="Enable Tensor Cores")
            tensor_check.set_active(self.app.config.get("enable_tensor_cores", True))
            tensor_check.set_sensitive(self.app.is_feature_available())
            box.pack_start(tensor_check, False, False, 5)
            
            precision_box = Gtk.Box(orientation=Gtk.Orientation.HORIZONTAL, spacing=10)
            precision_label = Gtk.Label(label="Mixed Precision:")
            precision_combo = Gtk.ComboBoxText()
            for p in ["fp16", "bf16", "tf32", "int8"]:
                precision_combo.append_text(p)
            precision_combo.set_active(0)
            precision_combo.set_sensitive(self.app.is_feature_available())
            precision_box.pack_start(precision_label, False, False, 0)
            precision_box.pack_start(precision_combo, False, False, 0)
            box.pack_start(precision_box, False, False, 5)
            
            strategy_box = Gtk.Box(orientation=Gtk.Orientation.HORIZONTAL, spacing=10)
            strategy_label = Gtk.Label(label="Multi-GPU Strategy:")
            strategy_combo = Gtk.ComboBoxText()
            for s in ["data_parallel", "model_parallel", "pipeline_parallel"]:
                strategy_combo.append_text(s)
            strategy_combo.set_active(0)
            strategy_combo.set_sensitive(self.app.is_feature_available())
            strategy_box.pack_start(strategy_label, False, False, 0)
            strategy_box.pack_start(strategy_combo, False, False, 0)
            box.pack_start(strategy_box, False, False, 5)
            
            return box
        
        def on_refresh(self, button):
            self.app.scan_hardware()
            dialog = Gtk.MessageDialog(
                transient_for=self,
                flags=0,
                message_type=Gtk.MessageType.INFO,
                buttons=Gtk.ButtonsType.OK,
                text="Hardware scan completed"
            )
            dialog.run()
            dialog.destroy()


def main():
    parser = argparse.ArgumentParser(description=f"{APP_NAME}")
    parser.add_argument("--gui", action="store_true", help="Launch GUI mode")
    parser.add_argument("--cli", action="store_true", help="Run in CLI mode")
    parser.add_argument("--scan", action="store_true", help="Scan hardware and output JSON")
    parser.add_argument("--status", action="store_true", help="Show unified API status")
    parser.add_argument("--backends", action="store_true", help="List available backends")
    parser.add_argument("--configure-tensor", nargs=2, metavar=("ENABLED", "PRECISION"),
                       help="Configure tensor cores (true/false, tf32/fp16/bf16/int8)")
    parser.add_argument("--framework", metavar="NAME", help="Get recommended backend for framework")
    parser.add_argument("--json", action="store_true", help="Output as JSON")
    parser.add_argument("--version", action="version", version=f"{APP_NAME} {VERSION}")
    
    args = parser.parse_args()
    
    if args.status:
        logging.disable(logging.CRITICAL)
        for handler in logging.root.handlers[:]:
            logging.root.removeHandler(handler)
        logging.root.addHandler(logging.NullHandler())
        try:
            app = AegisComputeStack(headless=True)
            caps = app.scan_hardware()
            gpu_list = []
            for gpu in caps.gpus:
                gpu_list.append({
                    "index": gpu.get("index", 0),
                    "name": gpu.get("name", "Unknown"),
                    "memory_mb": gpu.get("memory_total_mb", 0),
                    "backend": gpu.get("backend", "cpu")
                })
            status = {
                "available": True,
                "backends": caps.backends_available,
                "gpus": gpu_list,
                "version": VERSION
            }
            print(json.dumps(status))
            sys.exit(0)
        except Exception as e:
            print(json.dumps({"available": False, "error": str(e)}))
            sys.exit(1)
    elif args.scan:
        app = AegisComputeStack(headless=True)
        caps = app.scan_hardware()
        print(json.dumps({
            "backends_available": caps.backends_available,
            "primary_backend": caps.primary_backend,
            "gpus": caps.gpus,
            "tensor_cores_available": caps.tensor_cores_available,
            "mixed_precision_support": caps.mixed_precision_support,
            "multi_gpu_support": caps.multi_gpu_support,
            "total_vram_mb": caps.total_vram_mb,
            "cpu_cores": caps.cpu_cores,
            "cpu_threads": caps.cpu_threads
        }, indent=2))
    elif args.backends:
        app = AegisComputeStack(headless=True)
        backends = app.detect_all_backends()
        if args.json:
            print(json.dumps(backends, indent=2))
        else:
            for backend, available in backends.items():
                status = "✓" if available else "✗"
                print(f"{status} {backend.upper()}")
    elif args.configure_tensor:
        app = AegisComputeStack(headless=True)
        enabled = args.configure_tensor[0].lower() == "true"
        result = app.configure_tensor_cores(enabled, args.configure_tensor[1])
        print(json.dumps(result, indent=2))
    elif args.framework:
        app = AegisComputeStack(headless=True)
        backend = app.get_backend_for_framework(args.framework)
        print(f"Recommended backend for {args.framework}: {backend}")
    elif args.cli or not GTK_AVAILABLE:
        app = AegisComputeStack(headless=False)
        app.run_cli()
    else:
        app = AegisComputeStack(headless=False)
        app.run_gui()


if __name__ == "__main__":
    main()
