#!/usr/bin/env python3
"""
Aegis Compute Stack - Unified hardware acceleration API
Features: CUDA/ROCm/Vulkan/OpenCL detection, tensor-core config, unified API

Provides GUI (GTK) and CLI modes with tier-based feature gating.
"""

import os
import sys
import json
import subprocess
import logging
import argparse
import shutil
import re
from pathlib import Path
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, asdict
from enum import Enum

TIER_LIMIT = "aidev"
VERSION = "1.5.0"
APP_NAME = "Aegis Compute Stack"

CONFIG_FILE = "/etc/aegis/aidev-config.json"
LOG_FILE = "/var/log/aegis/compute-stack.log"

try:
    import gi
    gi.require_version('Gtk', '3.0')
    from gi.repository import Gtk, GLib, Pango
    GTK_AVAILABLE = True
except ImportError:
    GTK_AVAILABLE = False
    print("Error: GTK3 is required. Install with: sudo pacman -S gtk3 python-gobject", file=sys.stderr)


class LicenseTier:
    FREEMIUM = 1
    BASIC = 2
    AIDEV = 4
    SERVER = 5


class ComputeBackend(Enum):
    CUDA = "cuda"
    ROCM = "rocm"
    VULKAN = "vulkan"
    OPENCL = "opencl"
    CPU = "cpu"


@dataclass
class GPUDevice:
    index: int
    name: str
    vendor: str
    memory_total_mb: int
    memory_free_mb: int
    compute_capability: str
    driver_version: str
    backend: ComputeBackend
    tensor_cores: bool
    multi_instance_gpu: bool


@dataclass
class ComputeCapabilities:
    backends_available: List[str]
    primary_backend: str
    gpus: List[Dict]
    tensor_cores_available: bool
    mixed_precision_support: bool
    multi_gpu_support: bool
    total_vram_mb: int
    cpu_cores: int
    cpu_threads: int


class AegisComputeStack:
    def __init__(self, headless: bool = False):
        self.headless = headless
        self.version = VERSION
        self.config = {}
        self.license_tier = LicenseTier.FREEMIUM
        self.gpus: List[GPUDevice] = []
        self.primary_backend = ComputeBackend.CPU
        
        self.setup_logging()
        self.load_license_tier()
        self.load_config()
        
    def setup_logging(self):
        log_dir = Path(LOG_FILE).parent
        try:
            log_dir.mkdir(parents=True, exist_ok=True)
        except (PermissionError, OSError):
            pass
        
        try:
            logging.basicConfig(
                level=logging.INFO,
                format='%(asctime)s - %(levelname)s - [%(name)s] %(message)s',
                handlers=[
                    logging.FileHandler(LOG_FILE) if os.access(str(log_dir), os.W_OK) else logging.NullHandler(),
                    logging.StreamHandler() if not self.headless else logging.NullHandler()
                ]
            )
        except Exception:
            logging.basicConfig(level=logging.INFO, handlers=[logging.StreamHandler()])
        
        self.logger = logging.getLogger("AegisComputeStack")
        self.logger.info(f"Starting {APP_NAME} v{VERSION}")
    
    def load_license_tier(self):
        license_file = Path("/etc/aegis/license.json")
        try:
            if license_file.exists():
                with open(license_file, 'r') as f:
                    license_data = json.load(f)
                edition = license_data.get('edition', 'freemium').lower()
                tier_map = {
                    'freemium': LicenseTier.FREEMIUM,
                    'basic': LicenseTier.BASIC,
                    'aidev': LicenseTier.AIDEV,
                    'server': LicenseTier.SERVER
                }
                self.license_tier = tier_map.get(edition, LicenseTier.FREEMIUM)
            else:
                if Path("/etc/aegis-aidev-marker").exists():
                    self.license_tier = LicenseTier.AIDEV
        except Exception as e:
            self.logger.warning(f"Failed to load license tier: {e}")
    
    def is_feature_available(self) -> bool:
        return self.license_tier >= LicenseTier.AIDEV
    
    def load_config(self):
        default_config = {
            "preferred_backend": "auto",
            "gpu_power_limit": 100,
            "enable_tensor_cores": True,
            "mixed_precision": "fp16",
            "multi_gpu_strategy": "data_parallel"
        }
        
        try:
            if Path(CONFIG_FILE).exists():
                with open(CONFIG_FILE, 'r') as f:
                    file_config = json.load(f)
                    if "features" in file_config and "compute_stack" in file_config["features"]:
                        self.config = {**default_config, **file_config["features"]["compute_stack"]}
                    else:
                        self.config = default_config
            else:
                self.config = default_config
        except Exception as e:
            self.logger.error(f"Error loading config: {e}")
            self.config = default_config

    def check_nvidia_smi(self) -> bool:
        return shutil.which("nvidia-smi") is not None
    
    def check_nvcc(self) -> bool:
        return shutil.which("nvcc") is not None
    
    def check_rocm_smi(self) -> bool:
        return shutil.which("rocm-smi") is not None or shutil.which("rocminfo") is not None
    
    def check_vulkan(self) -> bool:
        return shutil.which("vulkaninfo") is not None or Path("/usr/share/vulkan").exists()
    
    def check_opencl(self) -> bool:
        return shutil.which("clinfo") is not None or Path("/etc/OpenCL/vendors").exists()

    def _detect_cuda(self) -> Dict[str, Any]:
        result = {
            "installed": False,
            "version": None,
            "nvcc_path": None,
            "cuda_home": None,
            "driver_version": None,
            "runtime_version": None
        }
        
        nvcc_path = shutil.which("nvcc")
        if nvcc_path:
            result["installed"] = True
            result["nvcc_path"] = nvcc_path
            
            try:
                proc = subprocess.run(
                    ["nvcc", "--version"],
                    capture_output=True, text=True, timeout=10
                )
                if proc.returncode == 0:
                    version_match = re.search(r'release\s+(\d+\.\d+)', proc.stdout)
                    if version_match:
                        result["version"] = version_match.group(1)
                    build_match = re.search(r'V(\d+\.\d+\.\d+)', proc.stdout)
                    if build_match:
                        result["runtime_version"] = build_match.group(1)
            except Exception as e:
                self.logger.error(f"Error getting CUDA version: {e}")
        
        cuda_paths = [
            os.environ.get("CUDA_HOME"),
            os.environ.get("CUDA_PATH"),
            "/usr/local/cuda",
            "/opt/cuda"
        ]
        for path in cuda_paths:
            if path and Path(path).exists():
                result["cuda_home"] = path
                if not result["installed"]:
                    nvcc_check = Path(path) / "bin" / "nvcc"
                    if nvcc_check.exists():
                        result["installed"] = True
                        result["nvcc_path"] = str(nvcc_check)
                break
        
        if self.check_nvidia_smi():
            try:
                proc = subprocess.run(
                    ["nvidia-smi", "--query-gpu=driver_version", "--format=csv,noheader"],
                    capture_output=True, text=True, timeout=10
                )
                if proc.returncode == 0 and proc.stdout.strip():
                    result["driver_version"] = proc.stdout.strip().split('\n')[0]
            except Exception:
                pass
        
        self.logger.info(f"CUDA detection: installed={result['installed']}, version={result['version']}")
        return result

    def _detect_rocm(self) -> Dict[str, Any]:
        result = {
            "installed": False,
            "version": None,
            "rocm_path": None,
            "hip_version": None,
            "amdgpu_driver": None
        }
        
        rocm_paths = [
            os.environ.get("ROCM_PATH"),
            "/opt/rocm",
            "/opt/rocm-*"
        ]
        
        for path_pattern in rocm_paths:
            if path_pattern:
                if '*' in path_pattern:
                    import glob
                    matches = sorted(glob.glob(path_pattern), reverse=True)
                    if matches:
                        result["rocm_path"] = matches[0]
                        result["installed"] = True
                        break
                elif Path(path_pattern).exists():
                    result["rocm_path"] = path_pattern
                    result["installed"] = True
                    break
        
        if result["rocm_path"]:
            version_file = Path(result["rocm_path"]) / ".info" / "version"
            if not version_file.exists():
                version_file = Path(result["rocm_path"]) / "version"
            if version_file.exists():
                try:
                    result["version"] = version_file.read_text().strip()
                except Exception:
                    pass
        
        rocminfo_path = shutil.which("rocminfo")
        if rocminfo_path:
            result["installed"] = True
            try:
                proc = subprocess.run(
                    ["rocminfo"],
                    capture_output=True, text=True, timeout=15
                )
                if proc.returncode == 0:
                    version_match = re.search(r'ROCm Runtime Version:\s*(\S+)', proc.stdout)
                    if version_match:
                        result["version"] = version_match.group(1)
            except Exception as e:
                self.logger.error(f"Error getting ROCm info: {e}")
        
        hipcc_path = shutil.which("hipcc")
        if hipcc_path:
            try:
                proc = subprocess.run(
                    ["hipcc", "--version"],
                    capture_output=True, text=True, timeout=10
                )
                if proc.returncode == 0:
                    hip_match = re.search(r'HIP version:\s*(\S+)', proc.stdout)
                    if hip_match:
                        result["hip_version"] = hip_match.group(1)
            except Exception:
                pass
        
        if shutil.which("modinfo"):
            try:
                proc = subprocess.run(
                    ["modinfo", "amdgpu"],
                    capture_output=True, text=True, timeout=10
                )
                if proc.returncode == 0:
                    version_match = re.search(r'version:\s*(\S+)', proc.stdout)
                    if version_match:
                        result["amdgpu_driver"] = version_match.group(1)
            except Exception:
                pass
        
        self.logger.info(f"ROCm detection: installed={result['installed']}, version={result['version']}")
        return result

    def _list_gpus(self) -> List[Dict[str, Any]]:
        gpus = []
        
        if self.check_nvidia_smi():
            try:
                proc = subprocess.run(
                    ["nvidia-smi", "--query-gpu=index,name,uuid,memory.total,memory.free,memory.used,utilization.gpu,temperature.gpu,power.draw,driver_version,compute_cap",
                     "--format=csv,noheader,nounits"],
                    capture_output=True, text=True, timeout=15
                )
                if proc.returncode == 0:
                    for line in proc.stdout.strip().split('\n'):
                        if not line.strip():
                            continue
                        parts = [p.strip() for p in line.split(',')]
                        if len(parts) >= 11:
                            gpu_info = {
                                "vendor": "NVIDIA",
                                "index": int(parts[0]),
                                "name": parts[1],
                                "uuid": parts[2],
                                "memory_total_mb": int(float(parts[3])) if parts[3] not in ['[N/A]', 'N/A'] else 0,
                                "memory_free_mb": int(float(parts[4])) if parts[4] not in ['[N/A]', 'N/A'] else 0,
                                "memory_used_mb": int(float(parts[5])) if parts[5] not in ['[N/A]', 'N/A'] else 0,
                                "gpu_utilization": int(float(parts[6])) if parts[6] not in ['[N/A]', 'N/A'] else 0,
                                "temperature_c": int(float(parts[7])) if parts[7] not in ['[N/A]', 'N/A'] else 0,
                                "power_draw_w": float(parts[8]) if parts[8] not in ['[N/A]', 'N/A'] else 0.0,
                                "driver_version": parts[9],
                                "compute_capability": parts[10],
                                "backend": "cuda"
                            }
                            gpus.append(gpu_info)
            except Exception as e:
                self.logger.error(f"Error listing NVIDIA GPUs: {e}")
        
        if self.check_rocm_smi():
            try:
                proc = subprocess.run(
                    ["rocm-smi", "--showallinfo", "--json"],
                    capture_output=True, text=True, timeout=15
                )
                if proc.returncode == 0:
                    try:
                        data = json.loads(proc.stdout)
                        for card_id, card_info in data.items():
                            if card_id.startswith("card"):
                                idx = int(card_id.replace("card", ""))
                                gpu_info = {
                                    "vendor": "AMD",
                                    "index": idx,
                                    "name": card_info.get("Card series", card_info.get("Card Model", "AMD GPU")),
                                    "uuid": card_info.get("Unique ID", ""),
                                    "memory_total_mb": int(card_info.get("VRAM Total Memory (B)", 0)) // (1024*1024),
                                    "memory_used_mb": int(card_info.get("VRAM Total Used Memory (B)", 0)) // (1024*1024),
                                    "memory_free_mb": 0,
                                    "gpu_utilization": int(card_info.get("GPU use (%)", 0)),
                                    "temperature_c": int(card_info.get("Temperature (Sensor edge) (C)", 0)),
                                    "power_draw_w": float(card_info.get("Average Graphics Package Power (W)", 0)),
                                    "driver_version": card_info.get("Driver version", "unknown"),
                                    "compute_capability": "gfx" + str(card_info.get("GPU ID", "unknown")),
                                    "backend": "rocm"
                                }
                                gpu_info["memory_free_mb"] = gpu_info["memory_total_mb"] - gpu_info["memory_used_mb"]
                                gpus.append(gpu_info)
                    except json.JSONDecodeError:
                        pass
            except subprocess.TimeoutExpired:
                pass
            except FileNotFoundError:
                pass
            except Exception as e:
                self.logger.error(f"Error listing AMD GPUs: {e}")
        
        self.logger.info(f"Listed {len(gpus)} GPU(s)")
        return gpus

    def _configure_cuda_visible_devices(self, device_ids: Optional[List[int]] = None, 
                                         uuid_list: Optional[List[str]] = None) -> Dict[str, Any]:
        result = {
            "success": False,
            "previous_value": os.environ.get("CUDA_VISIBLE_DEVICES"),
            "new_value": None,
            "message": ""
        }
        
        if not self.is_feature_available():
            result["message"] = "CUDA device configuration requires AI Developer edition"
            return result
        
        if device_ids is not None:
            value = ",".join(str(d) for d in device_ids)
        elif uuid_list is not None:
            value = ",".join(uuid_list)
        else:
            available_gpus = self._list_gpus()
            nvidia_gpus = [g for g in available_gpus if g["vendor"] == "NVIDIA"]
            if nvidia_gpus:
                value = ",".join(str(g["index"]) for g in nvidia_gpus)
            else:
                result["message"] = "No NVIDIA GPUs found to configure"
                return result
        
        os.environ["CUDA_VISIBLE_DEVICES"] = value
        result["success"] = True
        result["new_value"] = value
        result["message"] = f"CUDA_VISIBLE_DEVICES set to: {value}"
        
        self.logger.info(f"Configured CUDA_VISIBLE_DEVICES: {value}")
        return result

    def _check_gpu_memory(self, device_id: Optional[int] = None) -> Dict[str, Any]:
        result = {
            "success": False,
            "gpus": [],
            "total_memory_mb": 0,
            "total_used_mb": 0,
            "total_free_mb": 0
        }
        
        if self.check_nvidia_smi():
            try:
                cmd = ["nvidia-smi", "--query-gpu=index,name,memory.total,memory.used,memory.free",
                       "--format=csv,noheader,nounits"]
                if device_id is not None:
                    cmd.extend(["-i", str(device_id)])
                
                proc = subprocess.run(cmd, capture_output=True, text=True, timeout=10)
                if proc.returncode == 0:
                    for line in proc.stdout.strip().split('\n'):
                        if not line.strip():
                            continue
                        parts = [p.strip() for p in line.split(',')]
                        if len(parts) >= 5:
                            total = int(float(parts[2])) if parts[2] not in ['[N/A]', 'N/A'] else 0
                            used = int(float(parts[3])) if parts[3] not in ['[N/A]', 'N/A'] else 0
                            free = int(float(parts[4])) if parts[4] not in ['[N/A]', 'N/A'] else 0
                            
                            gpu_mem = {
                                "index": int(parts[0]),
                                "name": parts[1],
                                "vendor": "NVIDIA",
                                "memory_total_mb": total,
                                "memory_used_mb": used,
                                "memory_free_mb": free,
                                "memory_usage_percent": round((used / total * 100), 1) if total > 0 else 0
                            }
                            result["gpus"].append(gpu_mem)
                            result["total_memory_mb"] += total
                            result["total_used_mb"] += used
                            result["total_free_mb"] += free
                    result["success"] = True
            except Exception as e:
                self.logger.error(f"Error checking NVIDIA GPU memory: {e}")
        
        if self.check_rocm_smi():
            try:
                proc = subprocess.run(
                    ["rocm-smi", "--showmeminfo", "vram", "--json"],
                    capture_output=True, text=True, timeout=10
                )
                if proc.returncode == 0:
                    try:
                        data = json.loads(proc.stdout)
                        for card_id, card_info in data.items():
                            if card_id.startswith("card"):
                                idx = int(card_id.replace("card", ""))
                                if device_id is not None and idx != device_id:
                                    continue
                                
                                total = int(card_info.get("VRAM Total Memory (B)", 0)) // (1024*1024)
                                used = int(card_info.get("VRAM Total Used Memory (B)", 0)) // (1024*1024)
                                free = total - used
                                
                                gpu_mem = {
                                    "index": idx,
                                    "name": card_info.get("Card series", "AMD GPU"),
                                    "vendor": "AMD",
                                    "memory_total_mb": total,
                                    "memory_used_mb": used,
                                    "memory_free_mb": free,
                                    "memory_usage_percent": round((used / total * 100), 1) if total > 0 else 0
                                }
                                result["gpus"].append(gpu_mem)
                                result["total_memory_mb"] += total
                                result["total_used_mb"] += used
                                result["total_free_mb"] += free
                        result["success"] = True
                    except json.JSONDecodeError:
                        pass
            except Exception as e:
                self.logger.error(f"Error checking AMD GPU memory: {e}")
        
        if not result["gpus"]:
            result["message"] = "No GPUs found or unable to query memory"
        
        return result

    def _install_cuda_toolkit(self, version: Optional[str] = None) -> Dict[str, Any]:
        result = {
            "success": False,
            "method": None,
            "instructions": [],
            "message": ""
        }
        
        if not self.is_feature_available():
            result["message"] = "CUDA toolkit installation requires AI Developer edition"
            return result
        
        distro = None
        distro_version = None
        
        if Path("/etc/os-release").exists():
            try:
                with open("/etc/os-release", 'r') as f:
                    for line in f:
                        if line.startswith("ID="):
                            distro = line.split('=')[1].strip().strip('"')
                        elif line.startswith("VERSION_ID="):
                            distro_version = line.split('=')[1].strip().strip('"')
            except Exception:
                pass
        
        version_str = version if version else "latest"
        
        if distro in ["ubuntu", "debian"]:
            result["method"] = "apt"
            result["instructions"] = [
                "# Add NVIDIA CUDA repository",
                f"wget https://developer.download.nvidia.com/compute/cuda/repos/{distro}{distro_version.replace('.', '')}/x86_64/cuda-keyring_1.1-1_all.deb",
                "sudo dpkg -i cuda-keyring_1.1-1_all.deb",
                "sudo apt-get update",
                f"sudo apt-get install -y cuda-toolkit{'-' + version if version else ''}",
                "",
                "# Set environment variables",
                "export CUDA_HOME=/usr/local/cuda",
                "export PATH=$CUDA_HOME/bin:$PATH",
                "export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH"
            ]
            result["success"] = True
            result["message"] = f"Instructions generated for {distro} {distro_version}"
            
        elif distro in ["fedora", "rhel", "centos", "rocky", "almalinux"]:
            result["method"] = "dnf/yum"
            result["instructions"] = [
                "# Add NVIDIA CUDA repository",
                f"sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/{distro}{distro_version}/x86_64/cuda-{distro}{distro_version}.repo",
                "sudo dnf clean all",
                f"sudo dnf install -y cuda-toolkit{'-' + version if version else ''}",
                "",
                "# Set environment variables",
                "export CUDA_HOME=/usr/local/cuda",
                "export PATH=$CUDA_HOME/bin:$PATH",
                "export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH"
            ]
            result["success"] = True
            result["message"] = f"Instructions generated for {distro} {distro_version}"
            
        elif distro == "arch":
            result["method"] = "pacman"
            result["instructions"] = [
                "# Install CUDA from official repositories",
                "sudo pacman -S cuda cuda-tools",
                "",
                "# Set environment variables",
                "export CUDA_HOME=/opt/cuda",
                "export PATH=$CUDA_HOME/bin:$PATH",
                "export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH"
            ]
            result["success"] = True
            result["message"] = f"Instructions generated for Arch Linux"
            
        else:
            result["method"] = "manual"
            result["instructions"] = [
                "# Download CUDA Toolkit from NVIDIA website",
                "# Visit: https://developer.nvidia.com/cuda-downloads",
                "",
                "# For runfile installation:",
                f"wget https://developer.download.nvidia.com/compute/cuda/{version_str}/local_installers/cuda_{version_str}_linux.run",
                f"sudo sh cuda_{version_str}_linux.run",
                "",
                "# Set environment variables after installation",
                "export CUDA_HOME=/usr/local/cuda",
                "export PATH=$CUDA_HOME/bin:$PATH", 
                "export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH"
            ]
            result["success"] = True
            result["message"] = "Generic installation instructions generated"
        
        self.logger.info(f"Generated CUDA installation instructions for {distro or 'unknown'}")
        return result

    def detect_nvidia_gpus(self) -> List[GPUDevice]:
        gpus = []
        if not self.check_nvidia_smi():
            return gpus
        
        try:
            result = subprocess.run(
                ["nvidia-smi", "--query-gpu=index,name,memory.total,memory.free,driver_version,compute_cap", 
                 "--format=csv,noheader,nounits"],
                capture_output=True, text=True, timeout=10
            )
            
            if result.returncode == 0:
                for line in result.stdout.strip().split('\n'):
                    if not line.strip():
                        continue
                    parts = [p.strip() for p in line.split(',')]
                    if len(parts) >= 6:
                        compute_cap = parts[5]
                        major_ver = int(compute_cap.split('.')[0]) if '.' in compute_cap else int(compute_cap)
                        
                        gpu = GPUDevice(
                            index=int(parts[0]),
                            name=parts[1],
                            vendor="NVIDIA",
                            memory_total_mb=int(float(parts[2])),
                            memory_free_mb=int(float(parts[3])),
                            driver_version=parts[4],
                            compute_capability=compute_cap,
                            backend=ComputeBackend.CUDA,
                            tensor_cores=major_ver >= 7,
                            multi_instance_gpu=major_ver >= 8
                        )
                        gpus.append(gpu)
        except Exception as e:
            self.logger.error(f"Error detecting NVIDIA GPUs: {e}")
        
        return gpus
    
    def detect_amd_gpus(self) -> List[GPUDevice]:
        gpus = []
        if not self.check_rocm_smi():
            return gpus
        
        try:
            result = subprocess.run(
                ["rocm-smi", "--showid", "--showmeminfo", "vram", "--json"],
                capture_output=True, text=True, timeout=10
            )
            
            if result.returncode == 0:
                try:
                    data = json.loads(result.stdout)
                    for card_id, card_info in data.items():
                        if card_id.startswith("card"):
                            idx = int(card_id.replace("card", ""))
                            gpu = GPUDevice(
                                index=idx,
                                name=card_info.get("Card series", "AMD GPU"),
                                vendor="AMD",
                                memory_total_mb=int(card_info.get("VRAM Total Memory (B)", 0)) // (1024*1024),
                                memory_free_mb=int(card_info.get("VRAM Total Used Memory (B)", 0)) // (1024*1024),
                                driver_version=card_info.get("Driver version", "unknown"),
                                compute_capability="gfx" + card_info.get("GPU ID", "unknown"),
                                backend=ComputeBackend.ROCM,
                                tensor_cores=False,
                                multi_instance_gpu=False
                            )
                            gpus.append(gpu)
                except json.JSONDecodeError:
                    pass
        except subprocess.TimeoutExpired:
            pass
        except FileNotFoundError:
            pass
        except Exception as e:
            self.logger.error(f"Error detecting AMD GPUs: {e}")
        
        return gpus
    
    def detect_vulkan_devices(self) -> List[Dict[str, Any]]:
        devices = []
        if not self.check_vulkan():
            return devices
        
        try:
            result = subprocess.run(
                ["vulkaninfo", "--summary"],
                capture_output=True, text=True, timeout=10
            )
            
            if result.returncode == 0:
                lines = result.stdout.split('\n')
                current_device = {}
                for line in lines:
                    if "deviceName" in line:
                        if current_device:
                            devices.append(current_device)
                        current_device = {"name": line.split('=')[-1].strip()}
                    elif "deviceType" in line and current_device:
                        current_device["type"] = line.split('=')[-1].strip()
                    elif "apiVersion" in line and current_device:
                        current_device["api_version"] = line.split('=')[-1].strip()
                
                if current_device:
                    devices.append(current_device)
        except Exception as e:
            self.logger.error(f"Error detecting Vulkan devices: {e}")
        
        return devices
    
    def detect_opencl_devices(self) -> List[Dict[str, Any]]:
        devices = []
        if not self.check_opencl():
            return devices
        
        try:
            result = subprocess.run(
                ["clinfo", "-l"],
                capture_output=True, text=True, timeout=10
            )
            
            if result.returncode == 0:
                for line in result.stdout.split('\n'):
                    if 'Device' in line and 'Platform' not in line:
                        device_name = line.split(':')[-1].strip() if ':' in line else line.strip()
                        if device_name:
                            devices.append({"name": device_name, "type": "GPU/CPU"})
        except Exception as e:
            self.logger.error(f"Error detecting OpenCL devices: {e}")
        
        return devices

    def get_cpu_info(self) -> Dict[str, Any]:
        cpu_info = {"cores": 1, "threads": 1, "model": "Unknown"}
        
        try:
            if Path("/proc/cpuinfo").exists():
                with open("/proc/cpuinfo", 'r') as f:
                    content = f.read()
                    
                cores = content.count("processor")
                cpu_info["threads"] = cores if cores > 0 else 1
                
                for line in content.split('\n'):
                    if "model name" in line:
                        cpu_info["model"] = line.split(':')[-1].strip()
                        break
                    if "cpu cores" in line:
                        cpu_info["cores"] = int(line.split(':')[-1].strip())
        except Exception as e:
            self.logger.error(f"Error getting CPU info: {e}")
        
        return cpu_info

    def detect_all_backends(self) -> Dict[str, bool]:
        return {
            "cuda": self.check_nvidia_smi() or self.check_nvcc(),
            "rocm": self.check_rocm_smi(),
            "vulkan": self.check_vulkan(),
            "opencl": self.check_opencl(),
            "cpu": True
        }

    def scan_hardware(self) -> ComputeCapabilities:
        backends = self.detect_all_backends()
        available_backends = [k for k, v in backends.items() if v]
        
        self.gpus = []
        self.gpus.extend(self.detect_nvidia_gpus())
        self.gpus.extend(self.detect_amd_gpus())
        
        if backends["cuda"] and any(g.backend == ComputeBackend.CUDA for g in self.gpus):
            self.primary_backend = ComputeBackend.CUDA
        elif backends["rocm"] and any(g.backend == ComputeBackend.ROCM for g in self.gpus):
            self.primary_backend = ComputeBackend.ROCM
        elif backends["vulkan"]:
            self.primary_backend = ComputeBackend.VULKAN
        elif backends["opencl"]:
            self.primary_backend = ComputeBackend.OPENCL
        else:
            self.primary_backend = ComputeBackend.CPU
        
        cpu_info = self.get_cpu_info()
        
        total_vram = sum(g.memory_total_mb for g in self.gpus)
        tensor_cores = any(g.tensor_cores for g in self.gpus)
        
        capabilities = ComputeCapabilities(
            backends_available=available_backends,
            primary_backend=self.primary_backend.value,
            gpus=[asdict(g) for g in self.gpus],
            tensor_cores_available=tensor_cores,
            mixed_precision_support=tensor_cores or len(self.gpus) > 0,
            multi_gpu_support=len(self.gpus) > 1,
            total_vram_mb=total_vram,
            cpu_cores=cpu_info["cores"],
            cpu_threads=cpu_info["threads"]
        )
        
        return capabilities

    def get_unified_api_status(self) -> Dict[str, Any]:
        capabilities = self.scan_hardware()
        
        return {
            "version": self.version,
            "license_tier": "aidev" if self.is_feature_available() else "limited",
            "capabilities": asdict(capabilities) if hasattr(capabilities, '__dict__') else {
                "backends_available": capabilities.backends_available,
                "primary_backend": capabilities.primary_backend,
                "gpus": capabilities.gpus,
                "tensor_cores_available": capabilities.tensor_cores_available,
                "mixed_precision_support": capabilities.mixed_precision_support,
                "multi_gpu_support": capabilities.multi_gpu_support,
                "total_vram_mb": capabilities.total_vram_mb,
                "cpu_cores": capabilities.cpu_cores,
                "cpu_threads": capabilities.cpu_threads
            },
            "config": self.config
        }

    def configure_tensor_cores(self, enabled: bool = True, precision: str = "tf32") -> Dict[str, Any]:
        if not self.is_feature_available():
            return {"success": False, "error": "Tensor core configuration requires AI Developer edition"}
        
        if not any(g.tensor_cores for g in self.gpus):
            return {"success": False, "error": "No tensor cores detected"}
        
        valid_precisions = ["tf32", "fp16", "bf16", "int8"]
        if precision not in valid_precisions:
            return {"success": False, "error": f"Invalid precision. Use: {valid_precisions}"}
        
        self.config["enable_tensor_cores"] = enabled
        self.config["tensor_precision"] = precision
        
        self.logger.info(f"Tensor cores configured: enabled={enabled}, precision={precision}")
        return {"success": True, "enabled": enabled, "precision": precision}

    def set_gpu_power_limit(self, device_id: int, power_percent: int) -> Dict[str, Any]:
        if not self.is_feature_available():
            return {"success": False, "error": "Power management requires AI Developer edition"}
        
        if power_percent < 50 or power_percent > 100:
            return {"success": False, "error": "Power limit must be between 50-100%"}
        
        if self.check_nvidia_smi():
            try:
                result = subprocess.run(
                    ["nvidia-smi", "-i", str(device_id), "-pl", str(power_percent)],
                    capture_output=True, text=True, timeout=10
                )
                if result.returncode == 0:
                    return {"success": True, "device": device_id, "power_limit": power_percent}
            except Exception as e:
                return {"success": False, "error": str(e)}
        
        return {"success": False, "error": "Power limit setting not supported for this GPU"}

    def get_backend_for_framework(self, framework: str) -> str:
        framework_backends = {
            "pytorch": ["cuda", "rocm", "cpu"],
            "tensorflow": ["cuda", "rocm", "cpu"],
            "jax": ["cuda", "rocm", "cpu"],
            "onnx": ["cuda", "rocm", "cpu"],
            "triton": ["cuda"]
        }
        
        preferred = framework_backends.get(framework.lower(), ["cpu"])
        backends = self.detect_all_backends()
        
        for backend in preferred:
            if backends.get(backend, False):
                return backend
        
        return "cpu"

    def run_gui(self):
        if not GTK_AVAILABLE:
            return self.run_cli()
        
        win = ComputeStackWindow(self)
        win.connect("destroy", Gtk.main_quit)
        win.show_all()
        Gtk.main()
    
    def run_cli(self):
        print(f"\n{'='*70}")
        print(f"  {APP_NAME} v{VERSION}")
        print(f"  License: {'AI Developer Edition' if self.is_feature_available() else 'LIMITED'}")
        print(f"{'='*70}\n")
        
        status = self.get_unified_api_status()
        caps = status["capabilities"]
        
        print("Compute Backends:")
        for backend in caps["backends_available"]:
            primary = " (PRIMARY)" if backend == caps["primary_backend"] else ""
            print(f"  ✓ {backend.upper()}{primary}")
        
        print(f"\nGPU Devices ({len(caps['gpus'])} found):")
        if caps["gpus"]:
            for gpu in caps["gpus"]:
                tensor = " [Tensor Cores]" if gpu.get("tensor_cores") else ""
                print(f"  [{gpu['index']}] {gpu['name']} - {gpu['memory_total_mb']}MB VRAM{tensor}")
                print(f"      Driver: {gpu['driver_version']}, Compute: {gpu['compute_capability']}")
        else:
            print("  No GPU devices detected")
        
        print(f"\nCapabilities:")
        print(f"  Tensor Cores: {'✓' if caps['tensor_cores_available'] else '✗'}")
        print(f"  Mixed Precision: {'✓' if caps['mixed_precision_support'] else '✗'}")
        print(f"  Multi-GPU: {'✓' if caps['multi_gpu_support'] else '✗'}")
        print(f"  Total VRAM: {caps['total_vram_mb']} MB")
        print(f"  CPU: {caps['cpu_cores']} cores / {caps['cpu_threads']} threads")
        
        print(f"\nConfiguration:")
        for key, value in self.config.items():
            print(f"  {key}: {value}")


if GTK_AVAILABLE:
    class ComputeStackWindow(Gtk.Window):
        def __init__(self, app: AegisComputeStack):
            super().__init__(title=f"{APP_NAME} v{VERSION}")
            self.app = app
            self.set_default_size(900, 700)
            self.set_border_width(10)
            self.setup_ui()
        
        def setup_ui(self):
            vbox = Gtk.Box(orientation=Gtk.Orientation.VERTICAL, spacing=10)
            self.add(vbox)
            
            header = Gtk.Label()
            header.set_markup(f"<big><b>{APP_NAME}</b></big>")
            vbox.pack_start(header, False, False, 10)
            
            tier_label = Gtk.Label()
            tier_text = "AI Developer Edition" if self.app.is_feature_available() else "Limited Mode"
            tier_label.set_markup(f"<i>License: {tier_text}</i>")
            vbox.pack_start(tier_label, False, False, 5)
            
            notebook = Gtk.Notebook()
            vbox.pack_start(notebook, True, True, 0)
            
            notebook.append_page(self.create_overview_tab(), Gtk.Label(label="Overview"))
            notebook.append_page(self.create_gpus_tab(), Gtk.Label(label="GPU Devices"))
            notebook.append_page(self.create_backends_tab(), Gtk.Label(label="Backends"))
            notebook.append_page(self.create_config_tab(), Gtk.Label(label="Configuration"))
            
            refresh_btn = Gtk.Button(label="Refresh Hardware Scan")
            refresh_btn.connect("clicked", self.on_refresh)
            vbox.pack_start(refresh_btn, False, False, 10)
        
        def create_overview_tab(self):
            box = Gtk.Box(orientation=Gtk.Orientation.VERTICAL, spacing=10)
            box.set_margin_top(20)
            box.set_margin_start(20)
            
            status = self.app.get_unified_api_status()
            caps = status["capabilities"]
            
            grid = Gtk.Grid()
            grid.set_column_spacing(20)
            grid.set_row_spacing(10)
            
            labels = [
                ("Primary Backend:", caps["primary_backend"].upper()),
                ("Total VRAM:", f"{caps['total_vram_mb']} MB"),
                ("GPU Count:", str(len(caps["gpus"]))),
                ("Tensor Cores:", "Available" if caps["tensor_cores_available"] else "Not Available"),
                ("Mixed Precision:", "Supported" if caps["mixed_precision_support"] else "Not Supported"),
                ("Multi-GPU:", "Supported" if caps["multi_gpu_support"] else "Single GPU"),
                ("CPU Cores:", str(caps["cpu_cores"])),
                ("CPU Threads:", str(caps["cpu_threads"]))
            ]
            
            for i, (label, value) in enumerate(labels):
                lbl = Gtk.Label(label=label)
                lbl.set_xalign(0)
                val = Gtk.Label(label=value)
                val.set_xalign(0)
                grid.attach(lbl, 0, i, 1, 1)
                grid.attach(val, 1, i, 1, 1)
            
            box.pack_start(grid, False, False, 10)
            return box
        
        def create_gpus_tab(self):
            box = Gtk.Box(orientation=Gtk.Orientation.VERTICAL, spacing=10)
            box.set_margin_top(20)
            box.set_margin_start(20)
            
            status = self.app.get_unified_api_status()
            
            for gpu in status["capabilities"]["gpus"]:
                frame = Gtk.Frame(label=f"GPU {gpu['index']}: {gpu['name']}")
                frame_box = Gtk.Box(orientation=Gtk.Orientation.VERTICAL, spacing=5)
                frame_box.set_margin_top(10)
                frame_box.set_margin_start(10)
                frame_box.set_margin_bottom(10)
                
                info_labels = [
                    f"Vendor: {gpu['vendor']}",
                    f"VRAM: {gpu['memory_total_mb']} MB (Free: {gpu['memory_free_mb']} MB)",
                    f"Driver: {gpu['driver_version']}",
                    f"Compute Capability: {gpu['compute_capability']}",
                    f"Backend: {gpu['backend']}",
                    f"Tensor Cores: {'Yes' if gpu['tensor_cores'] else 'No'}"
                ]
                
                for info in info_labels:
                    lbl = Gtk.Label(label=info)
                    lbl.set_xalign(0)
                    frame_box.pack_start(lbl, False, False, 2)
                
                frame.add(frame_box)
                box.pack_start(frame, False, False, 10)
            
            if not status["capabilities"]["gpus"]:
                no_gpu = Gtk.Label(label="No GPU devices detected")
                box.pack_start(no_gpu, False, False, 20)
            
            return box
        
        def create_backends_tab(self):
            box = Gtk.Box(orientation=Gtk.Orientation.VERTICAL, spacing=10)
            box.set_margin_top(20)
            box.set_margin_start(20)
            
            backends = self.app.detect_all_backends()
            
            for backend, available in backends.items():
                hbox = Gtk.Box(orientation=Gtk.Orientation.HORIZONTAL, spacing=10)
                
                status_icon = "✓" if available else "✗"
                color = "green" if available else "red"
                label = Gtk.Label()
                label.set_markup(f"<span foreground='{color}'>{status_icon}</span> {backend.upper()}")
                label.set_xalign(0)
                hbox.pack_start(label, True, True, 10)
                
                box.pack_start(hbox, False, False, 5)
            
            return box
        
        def create_config_tab(self):
            box = Gtk.Box(orientation=Gtk.Orientation.VERTICAL, spacing=10)
            box.set_margin_top(20)
            box.set_margin_start(20)
            
            tensor_check = Gtk.CheckButton(label="Enable Tensor Cores")
            tensor_check.set_active(self.app.config.get("enable_tensor_cores", True))
            tensor_check.set_sensitive(self.app.is_feature_available())
            box.pack_start(tensor_check, False, False, 5)
            
            precision_box = Gtk.Box(orientation=Gtk.Orientation.HORIZONTAL, spacing=10)
            precision_label = Gtk.Label(label="Mixed Precision:")
            precision_combo = Gtk.ComboBoxText()
            for p in ["fp16", "bf16", "tf32", "int8"]:
                precision_combo.append_text(p)
            precision_combo.set_active(0)
            precision_combo.set_sensitive(self.app.is_feature_available())
            precision_box.pack_start(precision_label, False, False, 0)
            precision_box.pack_start(precision_combo, False, False, 0)
            box.pack_start(precision_box, False, False, 5)
            
            strategy_box = Gtk.Box(orientation=Gtk.Orientation.HORIZONTAL, spacing=10)
            strategy_label = Gtk.Label(label="Multi-GPU Strategy:")
            strategy_combo = Gtk.ComboBoxText()
            for s in ["data_parallel", "model_parallel", "pipeline_parallel"]:
                strategy_combo.append_text(s)
            strategy_combo.set_active(0)
            strategy_combo.set_sensitive(self.app.is_feature_available())
            strategy_box.pack_start(strategy_label, False, False, 0)
            strategy_box.pack_start(strategy_combo, False, False, 0)
            box.pack_start(strategy_box, False, False, 5)
            
            return box
        
        def on_refresh(self, button):
            self.app.scan_hardware()
            dialog = Gtk.MessageDialog(
                transient_for=self,
                flags=0,
                message_type=Gtk.MessageType.INFO,
                buttons=Gtk.ButtonsType.OK,
                text="Hardware scan completed"
            )
            dialog.run()
            dialog.destroy()


def main():
    if not GTK_AVAILABLE:
        print("Cannot start Compute Stack: GTK3 not available.", file=sys.stderr)
        sys.exit(1)
    parser = argparse.ArgumentParser(description=f"{APP_NAME}")
    parser.add_argument("--gui", action="store_true", help="Launch GUI mode")
    parser.add_argument("--cli", action="store_true", help="Run in CLI mode")
    parser.add_argument("--scan", action="store_true", help="Scan hardware and output JSON")
    parser.add_argument("--status", action="store_true", help="Show unified API status")
    parser.add_argument("--backends", action="store_true", help="List available backends")
    parser.add_argument("--configure-tensor", nargs=2, metavar=("ENABLED", "PRECISION"),
                       help="Configure tensor cores (true/false, tf32/fp16/bf16/int8)")
    parser.add_argument("--framework", metavar="NAME", help="Get recommended backend for framework")
    parser.add_argument("--json", action="store_true", help="Output as JSON")
    parser.add_argument("--version", action="version", version=f"{APP_NAME} {VERSION}")
    parser.add_argument("--detect-cuda", action="store_true", help="Detect CUDA installation and version")
    parser.add_argument("--detect-rocm", action="store_true", help="Detect ROCm installation and version")
    parser.add_argument("--list-gpus", action="store_true", help="List all available GPUs with details")
    parser.add_argument("--gpu-memory", action="store_true", help="Check GPU memory usage")
    parser.add_argument("--gpu-memory-device", type=int, metavar="ID", help="Check memory for specific GPU device")
    parser.add_argument("--set-cuda-devices", metavar="IDS", help="Set CUDA_VISIBLE_DEVICES (comma-separated IDs)")
    parser.add_argument("--install-cuda", action="store_true", help="Get CUDA toolkit installation instructions")
    parser.add_argument("--cuda-version", metavar="VER", help="CUDA version for installation instructions")
    
    args = parser.parse_args()
    
    if args.status:
        logging.disable(logging.CRITICAL)
        for handler in logging.root.handlers[:]:
            logging.root.removeHandler(handler)
        logging.root.addHandler(logging.NullHandler())
        try:
            app = AegisComputeStack(headless=True)
            caps = app.scan_hardware()
            gpu_list = []
            for gpu in caps.gpus:
                gpu_list.append({
                    "index": gpu.get("index", 0),
                    "name": gpu.get("name", "Unknown"),
                    "memory_mb": gpu.get("memory_total_mb", 0),
                    "backend": gpu.get("backend", "cpu")
                })
            status = {
                "available": True,
                "backends": caps.backends_available,
                "gpus": gpu_list,
                "version": VERSION
            }
            print(json.dumps(status))
            sys.exit(0)
        except Exception as e:
            print(json.dumps({"available": False, "error": str(e)}))
            sys.exit(1)
    elif args.scan:
        app = AegisComputeStack(headless=True)
        caps = app.scan_hardware()
        print(json.dumps({
            "backends_available": caps.backends_available,
            "primary_backend": caps.primary_backend,
            "gpus": caps.gpus,
            "tensor_cores_available": caps.tensor_cores_available,
            "mixed_precision_support": caps.mixed_precision_support,
            "multi_gpu_support": caps.multi_gpu_support,
            "total_vram_mb": caps.total_vram_mb,
            "cpu_cores": caps.cpu_cores,
            "cpu_threads": caps.cpu_threads
        }, indent=2))
    elif args.backends:
        app = AegisComputeStack(headless=True)
        backends = app.detect_all_backends()
        if args.json:
            print(json.dumps(backends, indent=2))
        else:
            for backend, available in backends.items():
                status = "✓" if available else "✗"
                print(f"{status} {backend.upper()}")
    elif args.configure_tensor:
        app = AegisComputeStack(headless=True)
        enabled = args.configure_tensor[0].lower() == "true"
        result = app.configure_tensor_cores(enabled, args.configure_tensor[1])
        print(json.dumps(result, indent=2))
    elif args.framework:
        app = AegisComputeStack(headless=True)
        backend = app.get_backend_for_framework(args.framework)
        print(f"Recommended backend for {args.framework}: {backend}")
    elif args.detect_cuda:
        app = AegisComputeStack(headless=True)
        result = app._detect_cuda()
        if args.json:
            print(json.dumps(result, indent=2))
        else:
            print(f"CUDA Installed: {'Yes' if result['installed'] else 'No'}")
            if result['version']:
                print(f"CUDA Version: {result['version']}")
            if result['runtime_version']:
                print(f"Runtime Version: {result['runtime_version']}")
            if result['nvcc_path']:
                print(f"NVCC Path: {result['nvcc_path']}")
            if result['cuda_home']:
                print(f"CUDA Home: {result['cuda_home']}")
            if result['driver_version']:
                print(f"Driver Version: {result['driver_version']}")
    elif args.detect_rocm:
        app = AegisComputeStack(headless=True)
        result = app._detect_rocm()
        if args.json:
            print(json.dumps(result, indent=2))
        else:
            print(f"ROCm Installed: {'Yes' if result['installed'] else 'No'}")
            if result['version']:
                print(f"ROCm Version: {result['version']}")
            if result['rocm_path']:
                print(f"ROCm Path: {result['rocm_path']}")
            if result['hip_version']:
                print(f"HIP Version: {result['hip_version']}")
            if result['amdgpu_driver']:
                print(f"AMDGPU Driver: {result['amdgpu_driver']}")
    elif args.list_gpus:
        app = AegisComputeStack(headless=True)
        gpus = app._list_gpus()
        if args.json:
            print(json.dumps(gpus, indent=2))
        else:
            if gpus:
                for gpu in gpus:
                    print(f"[{gpu['index']}] {gpu['vendor']} {gpu['name']}")
                    print(f"    UUID: {gpu.get('uuid', 'N/A')}")
                    print(f"    Memory: {gpu['memory_used_mb']}/{gpu['memory_total_mb']} MB ({gpu['memory_free_mb']} MB free)")
                    print(f"    Utilization: {gpu.get('gpu_utilization', 0)}%")
                    print(f"    Temperature: {gpu.get('temperature_c', 0)}°C")
                    print(f"    Power: {gpu.get('power_draw_w', 0):.1f}W")
                    print(f"    Driver: {gpu['driver_version']}")
                    print(f"    Compute: {gpu['compute_capability']}")
                    print(f"    Backend: {gpu['backend']}")
                    print()
            else:
                print("No GPUs detected")
    elif args.gpu_memory or args.gpu_memory_device is not None:
        app = AegisComputeStack(headless=True)
        device_id = args.gpu_memory_device if args.gpu_memory_device is not None else None
        result = app._check_gpu_memory(device_id)
        if args.json:
            print(json.dumps(result, indent=2))
        else:
            if result['success'] and result['gpus']:
                for gpu in result['gpus']:
                    print(f"[{gpu['index']}] {gpu['name']} ({gpu['vendor']})")
                    print(f"    Total: {gpu['memory_total_mb']} MB")
                    print(f"    Used:  {gpu['memory_used_mb']} MB ({gpu['memory_usage_percent']}%)")
                    print(f"    Free:  {gpu['memory_free_mb']} MB")
                    print()
                print(f"Total System GPU Memory: {result['total_memory_mb']} MB")
                print(f"Total Used: {result['total_used_mb']} MB")
                print(f"Total Free: {result['total_free_mb']} MB")
            else:
                print(result.get('message', 'Unable to check GPU memory'))
    elif args.set_cuda_devices:
        app = AegisComputeStack(headless=True)
        device_ids = [int(x.strip()) for x in args.set_cuda_devices.split(',')]
        result = app._configure_cuda_visible_devices(device_ids=device_ids)
        if args.json:
            print(json.dumps(result, indent=2))
        else:
            if result['success']:
                print(f"✓ {result['message']}")
                if result['previous_value']:
                    print(f"  Previous value: {result['previous_value']}")
            else:
                print(f"✗ Error: {result['message']}")
    elif args.install_cuda:
        app = AegisComputeStack(headless=True)
        result = app._install_cuda_toolkit(version=args.cuda_version)
        if args.json:
            print(json.dumps(result, indent=2))
        else:
            if result['success']:
                print(f"CUDA Toolkit Installation Instructions ({result['method']}):")
                print(f"Message: {result['message']}")
                print()
                for line in result['instructions']:
                    print(line)
            else:
                print(f"Error: {result['message']}")
    elif args.cli or not GTK_AVAILABLE:
        app = AegisComputeStack(headless=False)
        app.run_cli()
    else:
        app = AegisComputeStack(headless=False)
        app.run_gui()


if __name__ == "__main__":
    main()
